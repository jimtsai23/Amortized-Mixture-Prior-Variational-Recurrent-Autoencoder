{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.manifold import TSNE\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from duc import DUC\n",
    "from iaf.iaf_model import afpm\n",
    "from utils import linear_anneal, log_Normal_diag, log_Normal_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "max_len = 64\n",
    "batch_size = 32\n",
    "#splits = ['train', 'valid', 'test']\n",
    "\n",
    "# Penn TreeBank (PTB) dataset\n",
    "data_path = '../data'\n",
    "#datasets = {split: DUC(root=data_path, split=split) for split in splits}\n",
    "datasets = DUC(root=data_path, split='summ')\n",
    "\n",
    "# dataloader\n",
    "dataloaders = DataLoader(datasets,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    num_workers=cpu_count(),\n",
    "                                    pin_memory=torch.cuda.is_available())\n",
    "\n",
    "symbols = datasets.symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iaf model\n",
    "embedding_size = 300\n",
    "hidden_size = 256\n",
    "latent_dim = 32\n",
    "dropout_rate = 0.5\n",
    "model = afpm(vocab_size=datasets.vocab_size,\n",
    "               embed_size=embedding_size,\n",
    "               time_step=max_len,\n",
    "               hidden_size=hidden_size,\n",
    "               z_dim=latent_dim,\n",
    "               dropout_rate=dropout_rate,\n",
    "               bos_idx=symbols['<bos>'],\n",
    "               eos_idx=symbols['<eos>'],\n",
    "               pad_idx=symbols['<pad>'],\n",
    "               n_comb = 1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective function\n",
    "learning_rate = 0.001\n",
    "criterion = nn.NLLLoss(size_average=False, ignore_index=symbols['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# negative log likelihood\n",
    "def NLL(logp, target, length):\n",
    "    target = target[:, :torch.max(length).item()].contiguous().view(-1)\n",
    "    logp = logp.view(-1, logp.size(-1))\n",
    "    return criterion(logp, target)\n",
    "\n",
    "# KL divergence\n",
    "def KL_div(mu, logvar):\n",
    "    return -0.5 * torch.sum(1. + logvar - mu.pow(2) - logvar.exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training setting\n",
    "epoch = 20\n",
    "print_every = 50\n",
    "\n",
    "# training interface\n",
    "step = 0\n",
    "tracker = {'ELBO': [], 'NLL': [], 'KL': [], 'KL_weight': []}\n",
    "start_time = time.time()\n",
    "for ep in range(epoch):\n",
    "    # learning rate decay\n",
    "    if ep >= 10 and ep % 2 == 0:\n",
    "        learning_rate = learning_rate * 0.5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "    totals = {'ELBO': 0., 'NLL': 0., 'KL': 0., 'words': 0}\n",
    "    for itr, (enc_inputs, dec_inputs, targets, lengths, labels,_) in enumerate(dataloaders):\n",
    "        bsize = enc_inputs.size(0)\n",
    "        if True:\n",
    "            enc_inputs = enc_inputs.to(device)\n",
    "            dec_inputs = dec_inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        logp, z_0, z_T, mu, logvar, _ = model(enc_inputs, dec_inputs, lengths, labels)\n",
    "\n",
    "\n",
    "        # calculate loss\n",
    "        NLL_loss = NLL(logp, targets, lengths + 1)\n",
    "        # KL loss\n",
    "        log_p_z = log_Normal_standard(z_T, dim=1)\n",
    "        log_q_z = log_Normal_diag(z_0, mu, logvar, dim=1)\n",
    "        KL_loss = torch.sum(-(log_p_z - log_q_z))\n",
    "        KL_weight = linear_anneal(step, len(dataloaders) * 10)\n",
    "        loss = (NLL_loss + KL_weight * KL_loss) / bsize         \n",
    "\n",
    "\n",
    "        # cumulate\n",
    "        totals['ELBO'] += loss.item() * bsize\n",
    "        totals['NLL'] += NLL_loss.item()\n",
    "        totals['KL'] += KL_loss.item()\n",
    "        totals['words'] += torch.sum(lengths).item()\n",
    "\n",
    "        # backward and optimize\n",
    "        \n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n",
    "        # track\n",
    "        tracker['ELBO'].append(loss.item())\n",
    "        tracker['NLL'].append(NLL_loss.item() / bsize)\n",
    "        tracker['KL'].append(KL_loss.item() / bsize)\n",
    "        tracker['KL_weight'].append(KL_weight)\n",
    "\n",
    "        if(False):\n",
    "            # print statistics\n",
    "            if itr % print_every == 0 or itr + 1 == len(dataloaders):\n",
    "                print(\"summ Batch %04d/%04d, ELBO-Loss %.4f, \"\n",
    "                      \"NLL-Loss %.4f, KL-Loss %.4f, KL-Weight %.4f\"\n",
    "                      % ( itr, len(dataloaders),\n",
    "                         tracker['ELBO'][-1], tracker['NLL'][-1],\n",
    "                         tracker['KL'][-1], tracker['KL_weight'][-1]))\n",
    "        #break\n",
    "    #break\n",
    "\n",
    "    samples = len(datasets)\n",
    "    print(\"summ Epoch %02d/%02d, ELBO %.4f, NLL %.4f, KL %.4f, PPL %.4f\"\n",
    "          % ( ep, epoch, totals['ELBO'] / samples,\n",
    "             totals['NLL'] / samples, totals['KL'] / samples,\n",
    "             math.exp(totals['NLL'] / totals['words'])))\n",
    "    \n",
    "    #print('\\n')\n",
    "    #break\n",
    "    # save checkpoint\n",
    "    #checkpoint_path = os.path.join(save_path, \"E%02d.pkl\" % ep)\n",
    "    #torch.save(model.state_dict(), checkpoint_path)\n",
    "    #print(\"Model saved at %s\\n\" % checkpoint_path)\n",
    "end_time = time.time()\n",
    "print('Total cost time',\n",
    "      time.strftime(\"%H hr %M min %S sec\", time.gmtime(end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sent = []\n",
    "for i in range(1,46):\n",
    "    k = 0\n",
    "    for j in range(len(datasets)):\n",
    "        if datasets[j][4] == i:\n",
    "            k+=1\n",
    "    n_sent.append(k)\n",
    "\n",
    "all_sent = sum(n_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu\n",
    "torch.no_grad()\n",
    "a = 0\n",
    "b = 0\n",
    "id_all = []\n",
    "for i in range(45):\n",
    "    b += n_sent[i]\n",
    "    print(a,b)\n",
    "    \n",
    "    data = datasets[a:b]\n",
    "    a += n_sent[i]\n",
    "    loader = DataLoader(data, batch_size=n_sent[i])\n",
    "    enc_inputs, dec_inputs, targets, lengths, labels, sent_id = next(iter(loader))\n",
    "    _, sorted_idx = torch.sort(lengths, descending=True)\n",
    "    sent_id = np.array(sent_id)\n",
    "    sent_id = sent_id[sorted_idx]\n",
    "\n",
    "    enc_inputs = enc_inputs.to(device)\n",
    "    dec_inputs = dec_inputs.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    _, _, _, mu, _, _ = model(enc_inputs, dec_inputs, lengths, labels)\n",
    "\n",
    "    del enc_inputs\n",
    "    del dec_inputs\n",
    "    del lengths\n",
    "    del labels\n",
    "    #del logp\n",
    "    #del z\n",
    "    #del z_T\n",
    "    #del logvar\n",
    "    \n",
    "\n",
    "    z_mean = torch.sum(mu,0)\n",
    "    inner = []\n",
    "    for latent in mu:\n",
    "        inner.append(torch.abs(torch.dot(z_mean, latent) / torch.norm(z_mean) / torch.norm(latent)).item())\n",
    "    inner = np.array(inner)\n",
    "    inner_sort = np.argsort(-inner)\n",
    "    sent_id = sent_id[inner_sort]\n",
    "    id_all.append(sent_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('../data','summ')\n",
    "topics = os.listdir(path)\n",
    "topics.sort()\n",
    "i = 0\n",
    "summ = []\n",
    "for topic in topics:\n",
    "    i += 1\n",
    "    select = []\n",
    "    with open(os.path.join(path,topic),encoding='UTF-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for j in range(30):\n",
    "            identity = id_all[i-1][j]\n",
    "            select.append(lines[identity-1])\n",
    "    summ.append(select)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('../data','models')\n",
    "topics = os.listdir(path)\n",
    "topics.sort()\n",
    "\n",
    "g_truth = []\n",
    "for i in range(45):\n",
    "    ground = []\n",
    "    for j in range(4):\n",
    "        with open(os.path.join(path,topics[i*4+j]),encoding='ISO-8859-1') as f:\n",
    "            ground.append(f.read())\n",
    "    truth = ' '.join(ground)\n",
    "    g_truth.append(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(45):\n",
    "    hypo = ' '.join(summ[i])\n",
    "    score = []\n",
    "    \n",
    "    ref = g_truth[i]\n",
    "    scores.append(rouge.get_scores(hypo, ref, avg=True))\n",
    "    #scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1f = 0\n",
    "r1p = 0\n",
    "r1r = 0\n",
    "r2f = 0\n",
    "r2p = 0\n",
    "r2r = 0\n",
    "rlongf = 0\n",
    "rlongp = 0\n",
    "rlongr = 0\n",
    "for i in range(45):\n",
    "    r1f += scores[i]['rouge-1']['f'] * n_sent[i] / all_sent\n",
    "    r1p += scores[i]['rouge-1']['p'] * n_sent[i] / all_sent\n",
    "    r1r += scores[i]['rouge-1']['r'] * n_sent[i] / all_sent\n",
    "    r2f += scores[i]['rouge-2']['f'] * n_sent[i] / all_sent\n",
    "    r2p += scores[i]['rouge-2']['p'] * n_sent[i] / all_sent\n",
    "    r2r += scores[i]['rouge-2']['r'] * n_sent[i] / all_sent\n",
    "    rlongf += scores[i]['rouge-l']['f'] * n_sent[i] / all_sent\n",
    "    rlongp += scores[i]['rouge-l']['p'] * n_sent[i] / all_sent\n",
    "    rlongr += scores[i]['rouge-l']['r'] * n_sent[i] / all_sent\n",
    "print(r1f,r1p,r1r)\n",
    "print(r2f,r2p,r2r)\n",
    "print(rlongf,rlongp,rlongr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%.3f %.3f %.3f' % (r1f,r1p,r1r))\n",
    "print('%.3f %.3f %.3f' % (r2f,r2p,r2r))\n",
    "print('%.3f %.3f %.3f' % (rlongf,rlongp,rlongr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate au\n",
    "\n",
    "delta = 0.01\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    cnt = 0\n",
    "    for itr, (enc_inputs, dec_inputs, targets, lengths, labels,_) in enumerate(dataloaders):\n",
    "        bsize = enc_inputs.size(0)\n",
    "        if True:\n",
    "            enc_inputs = enc_inputs.to(device)\n",
    "            dec_inputs = dec_inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        logp, z_0, z_T, mu, logvar, _ = model(enc_inputs, dec_inputs, lengths, labels)\n",
    "        \n",
    "        if cnt == 0:\n",
    "            mu_sum = mu.sum(dim=0, keepdim=True)\n",
    "        else:\n",
    "            mu_sum = mu_sum + mu.sum(dim=0, keepdim=True)\n",
    "        cnt += mu.size(0)\n",
    "        \n",
    "    mu_mean = mu_sum / cnt\n",
    "        \n",
    "    cnt = 0\n",
    "    for itr, (enc_inputs, dec_inputs, targets, lengths, labels,_) in enumerate(dataloaders):\n",
    "        bsize = enc_inputs.size(0)\n",
    "        if True:\n",
    "            enc_inputs = enc_inputs.to(device)\n",
    "            dec_inputs = dec_inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            labels = labels.to(device)\n",
    "        # forward\n",
    "        logp, z_0, z_T, mu, logvar, _ = model(enc_inputs, dec_inputs, lengths, labels)\n",
    "        \n",
    "        if cnt == 0:\n",
    "            var_sum = ((mu - mu_mean) ** 2).sum(dim=0)\n",
    "        else:\n",
    "            var_sum = var_sum + ((mu - mu_mean) ** 2).sum(dim=0)\n",
    "        cnt += mu.size(0)\n",
    "        \n",
    "    au_var = var_sum / (cnt - 1)\n",
    "    \n",
    "    print((au_var >= delta).sum().item())\n",
    "    print(au_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot KL curve\n",
    "fig, ax1 = plt.subplots()\n",
    "lns1 = ax1.plot(tracker['KL_weight'], 'b', label='KL term weight')\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('KL term weight')\n",
    "ax2 = ax1.twinx()\n",
    "lns2 = ax2.plot(tracker['KL'], 'r', label='KL term value')\n",
    "ax2.set_ylabel('KL term value')\n",
    "lns = lns1 + lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax1.legend(lns, labs, bbox_to_anchor=(0., 1.02, 1., .102),\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent space visualization\n",
    "if(True):\n",
    "    features = np.empty([len(datasets), latent_dim])\n",
    "    feat_label = np.empty(len(datasets))\n",
    "    for itr, (enc_inputs, dec_inputs, _, lengths, labels,_) in enumerate(dataloaders):\n",
    "        enc_inputs = enc_inputs.to(device)\n",
    "        dec_inputs = dec_inputs.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        labels = labels.to(device)\n",
    "        _, _, _, mu, _, labels = model(enc_inputs, dec_inputs, lengths, labels)\n",
    "        start, end = batch_size * itr, batch_size * (itr + 1)\n",
    "        features[start:end] = mu.data.cpu().numpy()\n",
    "        feat_label[start:end] = labels.data.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_z = TSNE(n_components=2,perplexity=10).fit_transform(features)\n",
    "tracker['z'] = tsne_z\n",
    "tracker['label'] = feat_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(1,10):\n",
    "    plt.scatter(tsne_z[np.where(feat_label == i), 0], tsne_z[np.where(feat_label == i), 1], s=10, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save learning results\n",
    "sio.savemat(\"iaf_summ.mat\", tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
