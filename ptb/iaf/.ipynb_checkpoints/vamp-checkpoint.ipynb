{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.manifold import TSNE\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from ptb import PTB\n",
    "from model import vamp\n",
    "from utils import log_Normal_diag, linear_anneal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "max_len = 96\n",
    "batch_size = 32\n",
    "pseudo_size = 100\n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "# Penn TreeBank (PTB) dataset\n",
    "data_path = '../data'\n",
    "datasets = {split: PTB(root=data_path, split=split) for split in splits}\n",
    "pseudo_dataset = datasets['valid'][:pseudo_size]\n",
    "datasets['valid'] = datasets['valid'][pseudo_size:]\n",
    "\n",
    "# dataloader\n",
    "dataloaders = {split: DataLoader(datasets[split],\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=split=='train',\n",
    "                                    num_workers=cpu_count(),\n",
    "                                    pin_memory=torch.cuda.is_available())\n",
    "                                    for split in splits}\n",
    "\n",
    "symbols = datasets['train'].symbols\n",
    "\n",
    "pseudo_dataloader = DataLoader(pseudo_dataset,\n",
    "                                batch_size=pseudo_size,\n",
    "                                pin_memory=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamp model\n",
    "embedding_size = 300\n",
    "hidden_size = 256\n",
    "latent_dim = 16\n",
    "dropout_rate = 0.5\n",
    "model = vamp(vocab_size=datasets['train'].vocab_size,\n",
    "               embed_size=embedding_size,\n",
    "               time_step=max_len,\n",
    "               hidden_size=hidden_size,\n",
    "               z_dim=latent_dim,\n",
    "               dropout_rate=dropout_rate,\n",
    "               bos_idx=symbols['<bos>'],\n",
    "               eos_idx=symbols['<eos>'],\n",
    "               pad_idx=symbols['<pad>'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder to save model\n",
    "save_path = 'vamp'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo input\n",
    "pseudo_inputs, _, _, pseudo_lengths = next(iter(pseudo_dataloader))\n",
    "pseudo_inputs = pseudo_inputs.to(device)\n",
    "pseudo_lengths = pseudo_lengths.to(device)\n",
    "\n",
    "pseudo_sorted_len, pseudo_sorted_idx = torch.sort(pseudo_lengths, descending=True)\n",
    "pseudo_inputs = pseudo_inputs[pseudo_sorted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective function\n",
    "learning_rate = 0.001\n",
    "criterion = nn.NLLLoss(size_average=False, ignore_index=symbols['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# negative log likelihood\n",
    "def NLL(logp, target, length):\n",
    "    target = target[:, :torch.max(length).item()].contiguous().view(-1)\n",
    "    logp = logp.view(-1, logp.size(-1))\n",
    "    return criterion(logp, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(z_q):\n",
    "    z_p_mu, z_p_logvar = model.encoder(pseudo_inputs, pseudo_sorted_len)\n",
    "    z_q_expand = z_q.unsqueeze(1)\n",
    "    means = z_p_mu.unsqueeze(0)\n",
    "    logvars = z_p_logvar.unsqueeze(0)\n",
    "\n",
    "    a = log_Normal_diag(z_q_expand, means, logvars, dim=2) - math.log(pseudo_size)\n",
    "    a_max, _ = torch.max(a, 1)\n",
    "\n",
    "    log_prior = a_max + torch.log(torch.sum(torch.exp(a - a_max.unsqueeze(1)), 1))\n",
    "\n",
    "    \n",
    "    return log_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0000/1315, ELBO-Loss 235.3205, NLL-Loss 235.2222, KL-Loss 0.0983, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 125.7758, NLL-Loss 125.5807, KL-Loss 0.1950, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 137.8053, NLL-Loss 137.4756, KL-Loss 0.3297, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 162.7406, NLL-Loss 162.5983, KL-Loss 0.1423, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 124.7065, NLL-Loss 124.6512, KL-Loss 0.0553, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 129.5634, NLL-Loss 129.4680, KL-Loss 0.0954, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 124.9338, NLL-Loss 124.7888, KL-Loss 0.1450, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 121.6293, NLL-Loss 121.5720, KL-Loss 0.0573, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 108.2988, NLL-Loss 108.3019, KL-Loss -0.0031, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 127.1293, NLL-Loss 127.0354, KL-Loss 0.0938, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 137.8717, NLL-Loss 137.7397, KL-Loss 0.1320, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 122.4824, NLL-Loss 122.3786, KL-Loss 0.1037, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 129.7060, NLL-Loss 129.6597, KL-Loss 0.0463, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 117.4259, NLL-Loss 117.3292, KL-Loss 0.0967, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 127.0415, NLL-Loss 126.9963, KL-Loss 0.0452, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 114.0090, NLL-Loss 113.9516, KL-Loss 0.0574, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 133.5845, NLL-Loss 133.5168, KL-Loss 0.0677, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 138.5564, NLL-Loss 138.5265, KL-Loss 0.0299, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 127.4463, NLL-Loss 127.4990, KL-Loss -0.0527, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 128.8922, NLL-Loss 128.8614, KL-Loss 0.0308, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 101.3129, NLL-Loss 101.3295, KL-Loss -0.0167, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 122.6432, NLL-Loss 122.6272, KL-Loss 0.0160, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 124.9162, NLL-Loss 124.8744, KL-Loss 0.0418, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 104.1616, NLL-Loss 104.1016, KL-Loss 0.0600, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 133.9617, NLL-Loss 133.8445, KL-Loss 0.1172, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 132.8811, NLL-Loss 132.8799, KL-Loss 0.0012, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 125.1219, NLL-Loss 125.0676, KL-Loss 0.0543, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 108.6498, NLL-Loss 108.6673, KL-Loss -0.0175, KL-Weight 1.0000\n",
      "TRAIN Epoch 00/20, ELBO 129.5678, NLL 129.4948, KL 0.0730, PPL 463.1186\n",
      "VALID Epoch 00/20, ELBO 117.9175, NLL 117.8969, KL 0.0206, PPL 285.6407\n",
      "TEST Epoch 00/20, ELBO 117.4446, NLL 117.4158, KL 0.0288, PPL 274.0760\n",
      "Model saved at vamp06-02/1:21/E00.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 118.1811, NLL-Loss 118.1844, KL-Loss -0.0033, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 104.5673, NLL-Loss 104.5483, KL-Loss 0.0189, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 124.1189, NLL-Loss 124.1272, KL-Loss -0.0083, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 101.2801, NLL-Loss 101.2424, KL-Loss 0.0377, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 133.0046, NLL-Loss 132.9809, KL-Loss 0.0237, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 120.4672, NLL-Loss 120.4676, KL-Loss -0.0004, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 115.8407, NLL-Loss 115.7693, KL-Loss 0.0713, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 130.8843, NLL-Loss 130.8601, KL-Loss 0.0242, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 119.5739, NLL-Loss 119.5226, KL-Loss 0.0513, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 129.5549, NLL-Loss 129.5365, KL-Loss 0.0184, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 124.9691, NLL-Loss 124.9900, KL-Loss -0.0209, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 101.9715, NLL-Loss 101.9469, KL-Loss 0.0246, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 122.6539, NLL-Loss 122.6171, KL-Loss 0.0368, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 105.9437, NLL-Loss 105.8773, KL-Loss 0.0665, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 121.1405, NLL-Loss 121.1329, KL-Loss 0.0076, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 141.0398, NLL-Loss 141.0155, KL-Loss 0.0243, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 107.8074, NLL-Loss 107.8133, KL-Loss -0.0059, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 90.3614, NLL-Loss 90.3409, KL-Loss 0.0205, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 111.0867, NLL-Loss 111.0335, KL-Loss 0.0532, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 107.1086, NLL-Loss 107.0969, KL-Loss 0.0117, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 119.1467, NLL-Loss 119.1007, KL-Loss 0.0461, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 112.3973, NLL-Loss 112.3700, KL-Loss 0.0273, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 105.9803, NLL-Loss 105.9652, KL-Loss 0.0151, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 119.2804, NLL-Loss 119.2917, KL-Loss -0.0113, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 98.7476, NLL-Loss 98.7174, KL-Loss 0.0302, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 125.5696, NLL-Loss 125.5817, KL-Loss -0.0121, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 131.7451, NLL-Loss 131.7774, KL-Loss -0.0323, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 94.4178, NLL-Loss 94.3944, KL-Loss 0.0234, KL-Weight 1.0000\n",
      "TRAIN Epoch 01/20, ELBO 116.6297, NLL 116.6113, KL 0.0184, PPL 251.4665\n",
      "VALID Epoch 01/20, ELBO 113.0722, NLL 113.0561, KL 0.0161, PPL 226.4569\n",
      "TEST Epoch 01/20, ELBO 112.5793, NLL 112.5676, KL 0.0118, PPL 217.3740\n",
      "Model saved at vamp06-02/1:21/E01.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 116.3658, NLL-Loss 116.3605, KL-Loss 0.0053, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 129.7487, NLL-Loss 129.6791, KL-Loss 0.0696, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 115.4930, NLL-Loss 115.4837, KL-Loss 0.0093, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 121.8884, NLL-Loss 121.9072, KL-Loss -0.0189, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 130.6729, NLL-Loss 130.6312, KL-Loss 0.0416, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 122.4225, NLL-Loss 122.3463, KL-Loss 0.0762, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 94.3903, NLL-Loss 94.3907, KL-Loss -0.0004, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 117.1791, NLL-Loss 117.1813, KL-Loss -0.0022, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 128.2033, NLL-Loss 128.2242, KL-Loss -0.0208, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 105.6692, NLL-Loss 105.6382, KL-Loss 0.0310, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 118.3388, NLL-Loss 118.2963, KL-Loss 0.0425, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 120.7278, NLL-Loss 120.7133, KL-Loss 0.0146, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 107.6117, NLL-Loss 107.4953, KL-Loss 0.1164, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 99.0283, NLL-Loss 98.9828, KL-Loss 0.0455, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 109.4128, NLL-Loss 109.3983, KL-Loss 0.0146, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 91.9686, NLL-Loss 91.9434, KL-Loss 0.0251, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 101.9930, NLL-Loss 101.9814, KL-Loss 0.0116, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 109.1331, NLL-Loss 109.0905, KL-Loss 0.0427, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 112.1536, NLL-Loss 112.1918, KL-Loss -0.0382, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 112.9006, NLL-Loss 112.9085, KL-Loss -0.0079, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 121.3706, NLL-Loss 121.4096, KL-Loss -0.0391, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 141.0691, NLL-Loss 141.0641, KL-Loss 0.0050, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 96.8913, NLL-Loss 96.8681, KL-Loss 0.0232, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 105.1784, NLL-Loss 105.1724, KL-Loss 0.0060, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 96.9144, NLL-Loss 96.9178, KL-Loss -0.0033, KL-Weight 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1250/1315, ELBO-Loss 109.5657, NLL-Loss 109.5285, KL-Loss 0.0372, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 110.4987, NLL-Loss 110.4934, KL-Loss 0.0053, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 101.7718, NLL-Loss 101.7691, KL-Loss 0.0028, KL-Weight 1.0000\n",
      "TRAIN Epoch 02/20, ELBO 111.6781, NLL 111.6666, KL 0.0116, PPL 198.9262\n",
      "VALID Epoch 02/20, ELBO 110.9197, NLL 110.9132, KL 0.0066, PPL 204.3372\n",
      "TEST Epoch 02/20, ELBO 110.4896, NLL 110.4768, KL 0.0128, PPL 196.6971\n",
      "Model saved at vamp06-02/1:21/E02.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 107.6467, NLL-Loss 107.6245, KL-Loss 0.0222, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 104.9821, NLL-Loss 104.9645, KL-Loss 0.0176, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 124.0367, NLL-Loss 124.0077, KL-Loss 0.0289, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 94.7852, NLL-Loss 94.7882, KL-Loss -0.0030, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 119.1781, NLL-Loss 119.1741, KL-Loss 0.0040, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 111.7780, NLL-Loss 111.8400, KL-Loss -0.0620, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 107.7058, NLL-Loss 107.7013, KL-Loss 0.0045, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 103.5705, NLL-Loss 103.5870, KL-Loss -0.0165, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 111.4157, NLL-Loss 111.4200, KL-Loss -0.0044, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 109.2683, NLL-Loss 109.2757, KL-Loss -0.0074, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 102.8813, NLL-Loss 102.8265, KL-Loss 0.0549, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 116.7231, NLL-Loss 116.7414, KL-Loss -0.0182, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 99.4748, NLL-Loss 99.4876, KL-Loss -0.0128, KL-Weight 1.0000\n"
     ]
    }
   ],
   "source": [
    "# training setting\n",
    "epoch = 20\n",
    "print_every = 50\n",
    "\n",
    "# training interface\n",
    "step = 0\n",
    "tracker = {'ELBO': [], 'NLL': [], 'KL': [], 'KL_weight': []}\n",
    "start_time = time.time()\n",
    "for ep in range(epoch):\n",
    "    # learning rate decay\n",
    "    if ep >= 10 and ep % 2 == 0:\n",
    "        learning_rate = learning_rate * 0.5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "    for split in splits:\n",
    "        dataloader = dataloaders[split]\n",
    "        model.train() if split == 'train' else model.eval()\n",
    "        totals = {'ELBO': 0., 'NLL': 0., 'KL': 0., 'words': 0}\n",
    "\n",
    "        for itr, (enc_inputs, dec_inputs, targets, lengths) in enumerate(dataloader):\n",
    "            bsize = enc_inputs.size(0)\n",
    "            enc_inputs = enc_inputs.to(device)\n",
    "            dec_inputs = dec_inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            # forward\n",
    "            logp, z_q, mu, logvar = model(enc_inputs, dec_inputs, lengths)\n",
    "\n",
    "            # calculate loss\n",
    "            NLL_loss = NLL(logp, targets, lengths + 1)\n",
    "            # KL loss\n",
    "            log_p_z = log_prior(z_q)\n",
    "            log_q_z = log_Normal_diag(z_q, mu, logvar, dim=1)\n",
    "            KL_loss = torch.sum(-(log_p_z - log_q_z))\n",
    "            #KL_weight = linear_anneal(step, len(dataloaders['train']) * 10)\n",
    "            KL_weight = 1\n",
    "            loss = (NLL_loss + KL_weight * KL_loss) / bsize\n",
    "            \n",
    "            # cumulate\n",
    "            totals['ELBO'] += loss.item() * bsize\n",
    "            totals['NLL'] += NLL_loss.item()\n",
    "            totals['KL'] += KL_loss.item()\n",
    "            totals['words'] += torch.sum(lengths).item()\n",
    "\n",
    "            # backward and optimize\n",
    "            if split == 'train':\n",
    "                step += 1\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "                optimizer.step()\n",
    "\n",
    "                # track\n",
    "                tracker['ELBO'].append(loss.item())\n",
    "                tracker['NLL'].append(NLL_loss.item() / bsize)\n",
    "                tracker['KL'].append(KL_loss.item() / bsize)\n",
    "                tracker['KL_weight'].append(KL_weight)\n",
    "\n",
    "                # print statistics\n",
    "                if itr % print_every == 0 or itr + 1 == len(dataloader):\n",
    "                    print(\"%s Batch %04d/%04d, ELBO-Loss %.4f, \"\n",
    "                          \"NLL-Loss %.4f, KL-Loss %.4f, KL-Weight %.4f\"\n",
    "                          % (split.upper(), itr, len(dataloader),\n",
    "                             tracker['ELBO'][-1], tracker['NLL'][-1],\n",
    "                             tracker['KL'][-1], tracker['KL_weight'][-1]))\n",
    "\n",
    "        samples = len(datasets[split])\n",
    "        print(\"%s Epoch %02d/%02d, ELBO %.4f, NLL %.4f, KL %.4f, PPL %.4f\"\n",
    "              % (split.upper(), ep, epoch, totals['ELBO'] / samples,\n",
    "                 totals['NLL'] / samples, totals['KL'] / samples,\n",
    "                 math.exp(totals['NLL'] / totals['words'])))\n",
    "\n",
    "    # save checkpoint\n",
    "    checkpoint_path = os.path.join(save_path, \"E%02d.pkl\" % ep)\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(\"Model saved at %s\\n\" % checkpoint_path)\n",
    "end_time = time.time()\n",
    "print('Total cost time',\n",
    "      time.strftime(\"%H hr %M min %S sec\", time.gmtime(end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PPL %.4f' % (math.exp(totals['NLL']/totals['words'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KL %.4f' % (totals['KL'] / samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot KL curve\n",
    "fig, ax1 = plt.subplots()\n",
    "lns1 = ax1.plot(tracker['KL_weight'], 'b', label='KL term weight')\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('KL term weight')\n",
    "ax2 = ax1.twinx()\n",
    "lns2 = ax2.plot(tracker['KL'], 'r', label='KL term value')\n",
    "ax2.set_ylabel('KL term value')\n",
    "lns = lns1 + lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax1.legend(lns, labs, bbox_to_anchor=(0., 1.02, 1., .102),\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent space visualization\n",
    "features = np.empty([len(datasets['test']), latent_dim])\n",
    "gz = np.empty([len(datasets['test']), latent_dim])\n",
    "for itr, (enc_inputs, dec_inputs, _, lengths) in enumerate(dataloaders['test']):\n",
    "    enc_inputs = enc_inputs.to(device)\n",
    "    dec_inputs = dec_inputs.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    _, z, mu, _ = model(enc_inputs, dec_inputs, lengths)\n",
    "    start, end = batch_size * itr, batch_size * (itr + 1)\n",
    "    features[start:end] = mu.data.cpu().numpy()\n",
    "    start, end = batch_size * itr, batch_size * (itr + 1)\n",
    "    gz[start:end] = z.data.cpu().numpy()\n",
    "    \n",
    "tsne_z = TSNE(n_components=2).fit_transform(features)\n",
    "tracker['z'] = tsne_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(tsne_z[:, 0], tsne_z[:, 1], s=10, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save learning results\n",
    "sio.savemat(\"vanilla.mat\", tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
