{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.manifold import TSNE\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from ptb import PTB\n",
    "from model import vamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "max_len = 96\n",
    "batch_size = 32\n",
    "pseudo_size = 60\n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "# Penn TreeBank (PTB) dataset\n",
    "data_path = 'datasets/ptb'\n",
    "datasets = {split: PTB(root=data_path, split=split) for split in splits}\n",
    "pseudo_dataset = datasets['valid'][:pseudo_size]\n",
    "datasets['valid'] = datasets['valid'][pseudo_size:]\n",
    "\n",
    "# dataloader\n",
    "dataloaders = {split: DataLoader(datasets[split],\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=split=='train',\n",
    "                                    num_workers=cpu_count(),\n",
    "                                    pin_memory=torch.cuda.is_available())\n",
    "                                    for split in splits}\n",
    "\n",
    "symbols = datasets['train'].symbols\n",
    "\n",
    "pseudo_dataloader = DataLoader(pseudo_dataset,\n",
    "                                batch_size=pseudo_size,\n",
    "                                pin_memory=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vamp(\n",
      "  (encoder): LSTMEncoder(\n",
      "    (embedding): Embedding(10002, 300, padding_idx=0)\n",
      "    (rnn): LSTM(300, 256, batch_first=True)\n",
      "    (output): Linear(in_features=512, out_features=32, bias=True)\n",
      "  )\n",
      "  (embedding): Embedding(10002, 300, padding_idx=0)\n",
      "  (init_h): Linear(in_features=16, out_features=256, bias=True)\n",
      "  (init_c): Linear(in_features=16, out_features=256, bias=True)\n",
      "  (rnn): LSTM(300, 256, batch_first=True)\n",
      "  (output): Linear(in_features=256, out_features=10002, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# vamp model\n",
    "embedding_size = 300\n",
    "hidden_size = 256\n",
    "latent_dim = 16\n",
    "dropout_rate = 0.5\n",
    "model = vamp(vocab_size=datasets['train'].vocab_size,\n",
    "               embed_size=embedding_size,\n",
    "               time_step=max_len,\n",
    "               hidden_size=hidden_size,\n",
    "               z_dim=latent_dim,\n",
    "               dropout_rate=dropout_rate,\n",
    "               bos_idx=symbols['<bos>'],\n",
    "               eos_idx=symbols['<eos>'],\n",
    "               pad_idx=symbols['<pad>'])\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder to save model\n",
    "folder1 = str(datetime.datetime.now())[5:10]\n",
    "folder2 = str(datetime.datetime.now())[12:16]\n",
    "save_path = 'vamp' + folder1 + '/' + folder2\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo input\n",
    "pseudo_inputs, _, _, pseudo_lengths = next(iter(pseudo_dataloader))\n",
    "pseudo_inputs = pseudo_inputs.to(device)\n",
    "pseudo_lengths = pseudo_lengths.to(device)\n",
    "\n",
    "pseudo_sorted_len, pseudo_sorted_idx = torch.sort(pseudo_lengths, descending=True)\n",
    "pseudo_inputs = pseudo_inputs[pseudo_sorted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_Normal_diag(x, mean, log_var, dim=None):\n",
    "    log_normal = -0.5 * ( log_var + torch.pow( x - mean, 2 ) / torch.exp( log_var ) )\n",
    "\n",
    "    return torch.sum( log_normal, dim )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(z_q):\n",
    "    z_p_mu, z_p_logvar = model.encoder(pseudo_inputs, pseudo_sorted_len)\n",
    "    z_q_expand = z_q.unsqueeze(1)\n",
    "    means = z_p_mu.unsqueeze(0)\n",
    "    logvars = z_p_logvar.unsqueeze(0)\n",
    "\n",
    "    a = log_Normal_diag(z_q_expand, means, logvars, dim=2) - math.log(pseudo_size)\n",
    "    a_max, _ = torch.max(a, 1)\n",
    "\n",
    "    log_prior = a_max + torch.log(torch.sum(torch.exp(a - a_max.unsqueeze(1)), 1))\n",
    "\n",
    "    \n",
    "    return log_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective function\n",
    "learning_rate = 0.001\n",
    "criterion = nn.NLLLoss(size_average=False, ignore_index=symbols['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# negative log likelihood\n",
    "def NLL(logp, target, length):\n",
    "    target = target[:, :torch.max(length).item()].contiguous().view(-1)\n",
    "    logp = logp.view(-1, logp.size(-1))\n",
    "    return criterion(logp, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0000/1315, ELBO-Loss 184.3753, NLL-Loss 183.8268, KL-Loss 0.5485, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 164.9647, NLL-Loss 164.5995, KL-Loss 0.3652, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 174.7976, NLL-Loss 174.5412, KL-Loss 0.2564, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 139.1216, NLL-Loss 138.9873, KL-Loss 0.1343, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 156.9058, NLL-Loss 156.5879, KL-Loss 0.3178, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 131.2534, NLL-Loss 130.8858, KL-Loss 0.3675, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 122.1620, NLL-Loss 122.1517, KL-Loss 0.0103, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 143.0893, NLL-Loss 142.9825, KL-Loss 0.1069, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 138.0026, NLL-Loss 137.9836, KL-Loss 0.0190, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 135.6947, NLL-Loss 135.5991, KL-Loss 0.0957, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 127.1087, NLL-Loss 127.0526, KL-Loss 0.0562, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 103.7234, NLL-Loss 103.6355, KL-Loss 0.0879, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 132.8712, NLL-Loss 132.7912, KL-Loss 0.0799, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 140.8110, NLL-Loss 140.7815, KL-Loss 0.0296, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 136.7070, NLL-Loss 136.8048, KL-Loss -0.0978, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 116.4081, NLL-Loss 116.3303, KL-Loss 0.0778, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 115.2685, NLL-Loss 115.1382, KL-Loss 0.1302, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 130.5115, NLL-Loss 130.4730, KL-Loss 0.0386, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 138.2037, NLL-Loss 138.2424, KL-Loss -0.0387, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 141.9562, NLL-Loss 141.8482, KL-Loss 0.1080, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 136.4076, NLL-Loss 136.3960, KL-Loss 0.0117, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 119.9342, NLL-Loss 119.9140, KL-Loss 0.0202, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 117.6774, NLL-Loss 117.7124, KL-Loss -0.0350, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 138.6787, NLL-Loss 138.6512, KL-Loss 0.0276, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 137.3499, NLL-Loss 137.3395, KL-Loss 0.0105, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 121.4839, NLL-Loss 121.5016, KL-Loss -0.0177, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 109.5658, NLL-Loss 109.5460, KL-Loss 0.0198, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 122.5893, NLL-Loss 122.5795, KL-Loss 0.0098, KL-Weight 1.0000\n",
      "TRAIN Epoch 00/20, ELBO 129.6057, NLL 129.5319, KL 129.5319, PPL 1.0035\n",
      "VALID Epoch 00/20, ELBO 117.9845, NLL 117.9660, KL 117.9660, PPL 1.0009\n",
      "TEST Epoch 00/20, ELBO 117.2475, NLL 117.2240, KL 117.2240, PPL 1.0011\n",
      "Model saved at vamp05-23/4:20/E00.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 114.4206, NLL-Loss 114.4420, KL-Loss -0.0214, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 115.9772, NLL-Loss 115.9858, KL-Loss -0.0086, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 123.4478, NLL-Loss 123.4232, KL-Loss 0.0246, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 127.7034, NLL-Loss 127.6874, KL-Loss 0.0160, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 97.7453, NLL-Loss 97.7091, KL-Loss 0.0362, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 109.8122, NLL-Loss 109.8002, KL-Loss 0.0120, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 125.5080, NLL-Loss 125.4974, KL-Loss 0.0107, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 123.1128, NLL-Loss 123.1299, KL-Loss -0.0171, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 122.4802, NLL-Loss 122.4518, KL-Loss 0.0284, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 117.2661, NLL-Loss 117.2541, KL-Loss 0.0120, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 118.4249, NLL-Loss 118.3981, KL-Loss 0.0267, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 127.8823, NLL-Loss 127.8576, KL-Loss 0.0247, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 120.7401, NLL-Loss 120.7141, KL-Loss 0.0260, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 130.3998, NLL-Loss 130.3813, KL-Loss 0.0185, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 132.6943, NLL-Loss 132.6431, KL-Loss 0.0511, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 118.7589, NLL-Loss 118.7477, KL-Loss 0.0112, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 117.9845, NLL-Loss 117.9654, KL-Loss 0.0191, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 119.9707, NLL-Loss 119.9361, KL-Loss 0.0346, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 114.7622, NLL-Loss 114.7231, KL-Loss 0.0391, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 114.0659, NLL-Loss 114.0742, KL-Loss -0.0083, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 118.1743, NLL-Loss 118.1816, KL-Loss -0.0074, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 100.8468, NLL-Loss 100.8281, KL-Loss 0.0187, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 116.2472, NLL-Loss 116.2131, KL-Loss 0.0341, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 111.1911, NLL-Loss 111.1945, KL-Loss -0.0034, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 123.4889, NLL-Loss 123.4482, KL-Loss 0.0407, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 125.4370, NLL-Loss 125.4372, KL-Loss -0.0002, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 104.2833, NLL-Loss 104.2964, KL-Loss -0.0132, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 119.9727, NLL-Loss 119.9667, KL-Loss 0.0060, KL-Weight 1.0000\n",
      "TRAIN Epoch 01/20, ELBO 116.5697, NLL 116.5510, KL 116.5510, PPL 1.0009\n",
      "VALID Epoch 01/20, ELBO 113.4194, NLL 113.4002, KL 113.4002, PPL 1.0009\n",
      "TEST Epoch 01/20, ELBO 112.5628, NLL 112.5483, KL 112.5483, PPL 1.0007\n",
      "Model saved at vamp05-23/4:20/E01.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 105.7842, NLL-Loss 105.7995, KL-Loss -0.0153, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 100.4401, NLL-Loss 100.4264, KL-Loss 0.0137, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 128.8241, NLL-Loss 128.8127, KL-Loss 0.0115, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 109.2823, NLL-Loss 109.2581, KL-Loss 0.0243, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 105.0719, NLL-Loss 105.1070, KL-Loss -0.0351, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 102.8513, NLL-Loss 102.7978, KL-Loss 0.0535, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 124.5001, NLL-Loss 124.4937, KL-Loss 0.0064, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 106.9786, NLL-Loss 106.9885, KL-Loss -0.0099, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 109.8560, NLL-Loss 109.8062, KL-Loss 0.0497, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 120.6588, NLL-Loss 120.6551, KL-Loss 0.0038, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 120.6709, NLL-Loss 120.6584, KL-Loss 0.0125, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 111.5385, NLL-Loss 111.4593, KL-Loss 0.0792, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 97.5706, NLL-Loss 97.5496, KL-Loss 0.0210, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 110.2015, NLL-Loss 110.1693, KL-Loss 0.0322, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 114.6704, NLL-Loss 114.6497, KL-Loss 0.0207, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 120.7648, NLL-Loss 120.7953, KL-Loss -0.0305, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 101.5273, NLL-Loss 101.5409, KL-Loss -0.0137, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 97.6514, NLL-Loss 97.6483, KL-Loss 0.0031, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 130.1380, NLL-Loss 130.1236, KL-Loss 0.0143, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 115.4556, NLL-Loss 115.4107, KL-Loss 0.0448, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 129.1891, NLL-Loss 129.1900, KL-Loss -0.0009, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 114.0081, NLL-Loss 113.9921, KL-Loss 0.0160, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 105.1499, NLL-Loss 105.1478, KL-Loss 0.0021, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 122.3455, NLL-Loss 122.3639, KL-Loss -0.0184, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 104.3867, NLL-Loss 104.3889, KL-Loss -0.0023, KL-Weight 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1250/1315, ELBO-Loss 113.4461, NLL-Loss 113.4658, KL-Loss -0.0197, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 117.6167, NLL-Loss 117.6112, KL-Loss 0.0056, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 109.4566, NLL-Loss 109.4566, KL-Loss 0.0000, KL-Weight 1.0000\n",
      "TRAIN Epoch 02/20, ELBO 111.6511, NLL 111.6392, KL 111.6392, PPL 1.0006\n",
      "VALID Epoch 02/20, ELBO 111.1442, NLL 111.1373, KL 111.1373, PPL 1.0003\n",
      "TEST Epoch 02/20, ELBO 110.3225, NLL 110.3116, KL 110.3116, PPL 1.0005\n",
      "Model saved at vamp05-23/4:20/E02.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 108.0777, NLL-Loss 108.0698, KL-Loss 0.0078, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 96.0417, NLL-Loss 96.0360, KL-Loss 0.0058, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 105.3644, NLL-Loss 105.3684, KL-Loss -0.0040, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 91.9215, NLL-Loss 91.9302, KL-Loss -0.0087, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 122.2751, NLL-Loss 122.2386, KL-Loss 0.0365, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 115.0509, NLL-Loss 115.0419, KL-Loss 0.0091, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 100.4888, NLL-Loss 100.5260, KL-Loss -0.0372, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 110.5007, NLL-Loss 110.4916, KL-Loss 0.0091, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 105.1016, NLL-Loss 105.0733, KL-Loss 0.0283, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 107.3777, NLL-Loss 107.3171, KL-Loss 0.0607, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 111.0080, NLL-Loss 111.0224, KL-Loss -0.0144, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 98.3721, NLL-Loss 98.3692, KL-Loss 0.0029, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 124.0912, NLL-Loss 124.0631, KL-Loss 0.0281, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 133.3683, NLL-Loss 133.3653, KL-Loss 0.0029, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 121.4349, NLL-Loss 121.4091, KL-Loss 0.0257, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 108.5634, NLL-Loss 108.5460, KL-Loss 0.0175, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 122.8904, NLL-Loss 122.8913, KL-Loss -0.0009, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 97.3326, NLL-Loss 97.3362, KL-Loss -0.0037, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 108.8431, NLL-Loss 108.8485, KL-Loss -0.0055, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 100.5450, NLL-Loss 100.5412, KL-Loss 0.0038, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 116.6832, NLL-Loss 116.6646, KL-Loss 0.0186, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 108.1828, NLL-Loss 108.1653, KL-Loss 0.0175, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 127.0169, NLL-Loss 126.9982, KL-Loss 0.0187, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 107.4254, NLL-Loss 107.4309, KL-Loss -0.0055, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 94.3197, NLL-Loss 94.2755, KL-Loss 0.0442, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 111.8125, NLL-Loss 111.8112, KL-Loss 0.0012, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 112.2455, NLL-Loss 112.2603, KL-Loss -0.0148, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 108.4530, NLL-Loss 108.4898, KL-Loss -0.0368, KL-Weight 1.0000\n",
      "TRAIN Epoch 03/20, ELBO 108.3793, NLL 108.3701, KL 108.3701, PPL 1.0004\n",
      "VALID Epoch 03/20, ELBO 109.7504, NLL 109.7409, KL 109.7409, PPL 1.0005\n",
      "TEST Epoch 03/20, ELBO 108.9821, NLL 108.9782, KL 108.9782, PPL 1.0002\n",
      "Model saved at vamp05-23/4:20/E03.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 99.5856, NLL-Loss 99.5777, KL-Loss 0.0080, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 99.3847, NLL-Loss 99.3881, KL-Loss -0.0034, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 105.8038, NLL-Loss 105.8058, KL-Loss -0.0020, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 115.2020, NLL-Loss 115.2237, KL-Loss -0.0218, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 109.8469, NLL-Loss 109.8516, KL-Loss -0.0047, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 105.8421, NLL-Loss 105.8266, KL-Loss 0.0155, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 107.3835, NLL-Loss 107.3249, KL-Loss 0.0586, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 113.7007, NLL-Loss 113.6982, KL-Loss 0.0025, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 107.7565, NLL-Loss 107.7766, KL-Loss -0.0201, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 125.9999, NLL-Loss 126.0326, KL-Loss -0.0326, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 115.0024, NLL-Loss 115.0181, KL-Loss -0.0157, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 107.1403, NLL-Loss 107.1197, KL-Loss 0.0206, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 106.2137, NLL-Loss 106.1855, KL-Loss 0.0282, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 105.3195, NLL-Loss 105.3084, KL-Loss 0.0111, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 114.4708, NLL-Loss 114.4681, KL-Loss 0.0027, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 100.6007, NLL-Loss 100.6032, KL-Loss -0.0025, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 110.0199, NLL-Loss 110.0139, KL-Loss 0.0060, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 110.5207, NLL-Loss 110.4923, KL-Loss 0.0284, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 117.0568, NLL-Loss 117.0747, KL-Loss -0.0178, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 103.0858, NLL-Loss 103.1038, KL-Loss -0.0180, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 96.1468, NLL-Loss 96.1417, KL-Loss 0.0050, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 106.6541, NLL-Loss 106.6572, KL-Loss -0.0030, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 118.6376, NLL-Loss 118.6220, KL-Loss 0.0157, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 109.6534, NLL-Loss 109.6532, KL-Loss 0.0003, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 108.0989, NLL-Loss 108.0855, KL-Loss 0.0134, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 122.5342, NLL-Loss 122.5279, KL-Loss 0.0063, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 92.5921, NLL-Loss 92.6274, KL-Loss -0.0353, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 103.0020, NLL-Loss 102.9770, KL-Loss 0.0250, KL-Weight 1.0000\n",
      "TRAIN Epoch 04/20, ELBO 105.9124, NLL 105.9038, KL 105.9038, PPL 1.0004\n",
      "VALID Epoch 04/20, ELBO 108.9323, NLL 108.9266, KL 108.9266, PPL 1.0003\n",
      "TEST Epoch 04/20, ELBO 108.1343, NLL 108.1302, KL 108.1302, PPL 1.0002\n",
      "Model saved at vamp05-23/4:20/E04.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 102.3029, NLL-Loss 102.2918, KL-Loss 0.0111, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 102.0462, NLL-Loss 102.0363, KL-Loss 0.0098, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 106.3997, NLL-Loss 106.3775, KL-Loss 0.0222, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 86.0487, NLL-Loss 86.0578, KL-Loss -0.0091, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 102.7680, NLL-Loss 102.7785, KL-Loss -0.0106, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 106.7553, NLL-Loss 106.7629, KL-Loss -0.0076, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 102.1812, NLL-Loss 102.1804, KL-Loss 0.0008, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 130.0276, NLL-Loss 129.9928, KL-Loss 0.0349, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 88.3874, NLL-Loss 88.3807, KL-Loss 0.0067, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 104.4282, NLL-Loss 104.3767, KL-Loss 0.0515, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 86.8485, NLL-Loss 86.8419, KL-Loss 0.0067, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 118.1994, NLL-Loss 118.2042, KL-Loss -0.0048, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 110.4941, NLL-Loss 110.4896, KL-Loss 0.0045, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 103.1844, NLL-Loss 103.1658, KL-Loss 0.0185, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 91.1851, NLL-Loss 91.1732, KL-Loss 0.0119, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 107.1593, NLL-Loss 107.1672, KL-Loss -0.0079, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 99.6032, NLL-Loss 99.5825, KL-Loss 0.0207, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 90.7900, NLL-Loss 90.7864, KL-Loss 0.0036, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 98.4175, NLL-Loss 98.4009, KL-Loss 0.0166, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 104.5410, NLL-Loss 104.5490, KL-Loss -0.0080, KL-Weight 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1000/1315, ELBO-Loss 109.4971, NLL-Loss 109.5131, KL-Loss -0.0160, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 103.3644, NLL-Loss 103.1036, KL-Loss 0.2608, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 123.5181, NLL-Loss 123.5088, KL-Loss 0.0092, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 103.3207, NLL-Loss 103.2966, KL-Loss 0.0241, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 95.0282, NLL-Loss 94.9318, KL-Loss 0.0963, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 109.8624, NLL-Loss 109.8726, KL-Loss -0.0102, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 99.9247, NLL-Loss 99.9349, KL-Loss -0.0102, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 96.5926, NLL-Loss 96.6478, KL-Loss -0.0552, KL-Weight 1.0000\n",
      "TRAIN Epoch 05/20, ELBO 103.9959, NLL 103.9883, KL 103.9883, PPL 1.0004\n",
      "VALID Epoch 05/20, ELBO 108.4471, NLL 108.4410, KL 108.4410, PPL 1.0003\n",
      "TEST Epoch 05/20, ELBO 107.5488, NLL 107.5458, KL 107.5458, PPL 1.0001\n",
      "Model saved at vamp05-23/4:20/E05.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 106.1313, NLL-Loss 106.1098, KL-Loss 0.0216, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 105.5602, NLL-Loss 105.5612, KL-Loss -0.0010, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 105.3404, NLL-Loss 105.3362, KL-Loss 0.0042, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 110.3876, NLL-Loss 110.3780, KL-Loss 0.0096, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 98.9464, NLL-Loss 98.9406, KL-Loss 0.0058, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 98.8837, NLL-Loss 98.8963, KL-Loss -0.0126, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 102.4694, NLL-Loss 102.4454, KL-Loss 0.0240, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 113.5858, NLL-Loss 113.6046, KL-Loss -0.0188, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 95.7016, NLL-Loss 95.6989, KL-Loss 0.0027, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 114.7185, NLL-Loss 114.7446, KL-Loss -0.0261, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 108.9969, NLL-Loss 108.9937, KL-Loss 0.0032, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 87.1291, NLL-Loss 87.1075, KL-Loss 0.0216, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 102.0510, NLL-Loss 102.0522, KL-Loss -0.0012, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 94.3878, NLL-Loss 94.3544, KL-Loss 0.0334, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 83.1014, NLL-Loss 83.0947, KL-Loss 0.0067, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 87.6956, NLL-Loss 87.6807, KL-Loss 0.0150, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 105.4662, NLL-Loss 105.4619, KL-Loss 0.0042, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 105.4593, NLL-Loss 105.4431, KL-Loss 0.0162, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 87.3483, NLL-Loss 87.3249, KL-Loss 0.0235, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 108.5174, NLL-Loss 108.4964, KL-Loss 0.0209, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 99.0168, NLL-Loss 99.0125, KL-Loss 0.0043, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 110.4008, NLL-Loss 110.4051, KL-Loss -0.0043, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 104.9636, NLL-Loss 104.9515, KL-Loss 0.0121, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 100.7521, NLL-Loss 100.7558, KL-Loss -0.0037, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 106.8666, NLL-Loss 106.8435, KL-Loss 0.0231, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 116.0237, NLL-Loss 116.0163, KL-Loss 0.0074, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 119.5529, NLL-Loss 119.5399, KL-Loss 0.0130, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 90.0185, NLL-Loss 90.0139, KL-Loss 0.0046, KL-Weight 1.0000\n",
      "TRAIN Epoch 06/20, ELBO 102.3021, NLL 102.2948, KL 102.2948, PPL 1.0003\n",
      "VALID Epoch 06/20, ELBO 108.0878, NLL 108.0822, KL 108.0822, PPL 1.0003\n",
      "TEST Epoch 06/20, ELBO 107.0716, NLL 107.0640, KL 107.0640, PPL 1.0004\n",
      "Model saved at vamp05-23/4:20/E06.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 100.6526, NLL-Loss 100.6386, KL-Loss 0.0140, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 98.3098, NLL-Loss 98.3468, KL-Loss -0.0370, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 120.4952, NLL-Loss 120.4852, KL-Loss 0.0100, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 88.0833, NLL-Loss 88.0907, KL-Loss -0.0074, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 109.9206, NLL-Loss 109.9080, KL-Loss 0.0126, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 94.8018, NLL-Loss 94.7942, KL-Loss 0.0076, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 110.0620, NLL-Loss 110.0547, KL-Loss 0.0073, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 91.1729, NLL-Loss 91.1548, KL-Loss 0.0181, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 99.5505, NLL-Loss 99.5612, KL-Loss -0.0107, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 98.8408, NLL-Loss 98.8343, KL-Loss 0.0065, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 99.1056, NLL-Loss 99.0897, KL-Loss 0.0160, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 100.7191, NLL-Loss 100.6937, KL-Loss 0.0253, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 112.0125, NLL-Loss 111.9773, KL-Loss 0.0352, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 108.8237, NLL-Loss 108.8352, KL-Loss -0.0115, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 87.5976, NLL-Loss 87.5846, KL-Loss 0.0130, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 103.6851, NLL-Loss 103.6786, KL-Loss 0.0064, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 87.4606, NLL-Loss 87.4471, KL-Loss 0.0135, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 103.7307, NLL-Loss 103.7171, KL-Loss 0.0136, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 100.8370, NLL-Loss 100.8110, KL-Loss 0.0260, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 113.9511, NLL-Loss 113.9581, KL-Loss -0.0070, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 100.3286, NLL-Loss 100.3117, KL-Loss 0.0169, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 113.0698, NLL-Loss 113.0755, KL-Loss -0.0057, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 101.1470, NLL-Loss 101.1269, KL-Loss 0.0201, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 116.3578, NLL-Loss 116.3340, KL-Loss 0.0238, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 100.2313, NLL-Loss 100.2144, KL-Loss 0.0170, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 90.4243, NLL-Loss 90.4462, KL-Loss -0.0219, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 100.7693, NLL-Loss 100.7813, KL-Loss -0.0120, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 89.5921, NLL-Loss 89.5908, KL-Loss 0.0013, KL-Weight 1.0000\n",
      "TRAIN Epoch 07/20, ELBO 100.9349, NLL 100.9289, KL 100.9289, PPL 1.0003\n",
      "VALID Epoch 07/20, ELBO 107.9957, NLL 107.9896, KL 107.9896, PPL 1.0003\n",
      "TEST Epoch 07/20, ELBO 106.8938, NLL 106.8883, KL 106.8883, PPL 1.0003\n",
      "Model saved at vamp05-23/4:20/E07.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 115.0733, NLL-Loss 115.0545, KL-Loss 0.0188, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 96.2624, NLL-Loss 96.2721, KL-Loss -0.0097, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 111.4543, NLL-Loss 111.4439, KL-Loss 0.0104, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 107.6698, NLL-Loss 107.6539, KL-Loss 0.0159, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 99.1486, NLL-Loss 99.1275, KL-Loss 0.0211, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 90.4818, NLL-Loss 90.4820, KL-Loss -0.0002, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 103.6856, NLL-Loss 103.6783, KL-Loss 0.0072, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 110.3864, NLL-Loss 110.3830, KL-Loss 0.0034, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 103.1935, NLL-Loss 103.1732, KL-Loss 0.0203, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 108.4349, NLL-Loss 108.4135, KL-Loss 0.0214, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 111.2487, NLL-Loss 111.2397, KL-Loss 0.0090, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 96.5749, NLL-Loss 96.5724, KL-Loss 0.0025, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 93.9301, NLL-Loss 93.9392, KL-Loss -0.0091, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 104.0962, NLL-Loss 104.0969, KL-Loss -0.0007, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 101.6319, NLL-Loss 101.6289, KL-Loss 0.0030, KL-Weight 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0750/1315, ELBO-Loss 100.3171, NLL-Loss 100.3130, KL-Loss 0.0041, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 118.7701, NLL-Loss 118.7538, KL-Loss 0.0162, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 97.1925, NLL-Loss 97.1913, KL-Loss 0.0012, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 99.6967, NLL-Loss 99.6597, KL-Loss 0.0370, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 114.9266, NLL-Loss 114.9329, KL-Loss -0.0063, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 94.2052, NLL-Loss 94.1833, KL-Loss 0.0219, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 95.6328, NLL-Loss 95.6233, KL-Loss 0.0095, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 96.4053, NLL-Loss 96.3950, KL-Loss 0.0104, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 83.2927, NLL-Loss 83.2512, KL-Loss 0.0415, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 95.1096, NLL-Loss 95.0969, KL-Loss 0.0127, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 115.4080, NLL-Loss 115.3916, KL-Loss 0.0164, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 106.3316, NLL-Loss 106.3436, KL-Loss -0.0120, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 90.2062, NLL-Loss 90.2025, KL-Loss 0.0037, KL-Weight 1.0000\n",
      "TRAIN Epoch 08/20, ELBO 99.7048, NLL 99.6981, KL 99.6981, PPL 1.0003\n",
      "VALID Epoch 08/20, ELBO 107.8770, NLL 107.8720, KL 107.8720, PPL 1.0002\n",
      "TEST Epoch 08/20, ELBO 106.7061, NLL 106.7054, KL 106.7054, PPL 1.0000\n",
      "Model saved at vamp05-23/4:20/E08.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 112.3539, NLL-Loss 112.3603, KL-Loss -0.0064, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 85.3795, NLL-Loss 85.2598, KL-Loss 0.1197, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 102.6275, NLL-Loss 102.6230, KL-Loss 0.0045, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 99.3372, NLL-Loss 99.3227, KL-Loss 0.0146, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 85.6262, NLL-Loss 85.6241, KL-Loss 0.0021, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 113.3373, NLL-Loss 113.3369, KL-Loss 0.0004, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 105.8291, NLL-Loss 105.8342, KL-Loss -0.0051, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 105.7336, NLL-Loss 105.7143, KL-Loss 0.0192, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 104.7245, NLL-Loss 104.7353, KL-Loss -0.0108, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 111.4093, NLL-Loss 111.4089, KL-Loss 0.0003, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 81.7678, NLL-Loss 81.7674, KL-Loss 0.0004, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 88.1019, NLL-Loss 88.0902, KL-Loss 0.0117, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 102.3156, NLL-Loss 102.2813, KL-Loss 0.0343, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 105.1869, NLL-Loss 105.1780, KL-Loss 0.0088, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 99.6817, NLL-Loss 99.6863, KL-Loss -0.0046, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 93.3232, NLL-Loss 93.3268, KL-Loss -0.0035, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 92.4524, NLL-Loss 92.4602, KL-Loss -0.0078, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 98.8235, NLL-Loss 98.8052, KL-Loss 0.0183, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 94.5850, NLL-Loss 94.5836, KL-Loss 0.0013, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 101.8882, NLL-Loss 101.7946, KL-Loss 0.0936, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 96.6336, NLL-Loss 96.6168, KL-Loss 0.0168, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 74.4488, NLL-Loss 74.4216, KL-Loss 0.0272, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 115.0280, NLL-Loss 114.9966, KL-Loss 0.0314, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 107.6474, NLL-Loss 107.6341, KL-Loss 0.0133, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 111.7920, NLL-Loss 111.7925, KL-Loss -0.0005, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 89.6184, NLL-Loss 89.6130, KL-Loss 0.0054, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 106.0272, NLL-Loss 106.0189, KL-Loss 0.0083, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 96.0788, NLL-Loss 96.0882, KL-Loss -0.0094, KL-Weight 1.0000\n",
      "TRAIN Epoch 09/20, ELBO 98.6023, NLL 98.5971, KL 98.5971, PPL 1.0002\n",
      "VALID Epoch 09/20, ELBO 107.9352, NLL 107.9283, KL 107.9283, PPL 1.0003\n",
      "TEST Epoch 09/20, ELBO 106.7096, NLL 106.7054, KL 106.7054, PPL 1.0002\n",
      "Model saved at vamp05-23/4:20/E09.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 106.4979, NLL-Loss 106.4948, KL-Loss 0.0030, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 110.8788, NLL-Loss 110.8254, KL-Loss 0.0534, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 94.4234, NLL-Loss 94.4236, KL-Loss -0.0002, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 101.9635, NLL-Loss 101.9585, KL-Loss 0.0050, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 95.7601, NLL-Loss 95.7651, KL-Loss -0.0050, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 98.6704, NLL-Loss 98.6681, KL-Loss 0.0023, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 95.4515, NLL-Loss 95.4518, KL-Loss -0.0003, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 104.2080, NLL-Loss 104.2073, KL-Loss 0.0006, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 92.3392, NLL-Loss 92.3115, KL-Loss 0.0277, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 93.7020, NLL-Loss 93.7224, KL-Loss -0.0204, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 113.2226, NLL-Loss 113.2111, KL-Loss 0.0115, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 97.9303, NLL-Loss 97.9333, KL-Loss -0.0030, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 86.6949, NLL-Loss 86.7273, KL-Loss -0.0324, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 91.0402, NLL-Loss 91.0382, KL-Loss 0.0020, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 85.8152, NLL-Loss 85.8131, KL-Loss 0.0021, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 97.1365, NLL-Loss 97.1310, KL-Loss 0.0055, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 102.4233, NLL-Loss 102.4276, KL-Loss -0.0043, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 86.3712, NLL-Loss 86.3680, KL-Loss 0.0032, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 102.5461, NLL-Loss 102.5315, KL-Loss 0.0147, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 102.9043, NLL-Loss 102.9035, KL-Loss 0.0009, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 93.6744, NLL-Loss 93.6857, KL-Loss -0.0113, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 95.4584, NLL-Loss 95.4447, KL-Loss 0.0137, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 101.0635, NLL-Loss 101.0602, KL-Loss 0.0033, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 86.3886, NLL-Loss 86.3739, KL-Loss 0.0147, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 80.7493, NLL-Loss 80.7447, KL-Loss 0.0046, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 94.0747, NLL-Loss 94.0668, KL-Loss 0.0079, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 96.5622, NLL-Loss 96.5638, KL-Loss -0.0015, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 105.8255, NLL-Loss 105.8063, KL-Loss 0.0192, KL-Weight 1.0000\n",
      "TRAIN Epoch 10/20, ELBO 96.6751, NLL 96.6703, KL 96.6703, PPL 1.0002\n",
      "VALID Epoch 10/20, ELBO 107.7262, NLL 107.7235, KL 107.7235, PPL 1.0001\n",
      "TEST Epoch 10/20, ELBO 106.2833, NLL 106.2824, KL 106.2824, PPL 1.0000\n",
      "Model saved at vamp05-23/4:20/E10.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 79.5312, NLL-Loss 79.5327, KL-Loss -0.0014, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 98.8650, NLL-Loss 98.8619, KL-Loss 0.0031, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 95.9441, NLL-Loss 95.9210, KL-Loss 0.0231, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 84.4772, NLL-Loss 84.4769, KL-Loss 0.0003, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 101.2465, NLL-Loss 101.2451, KL-Loss 0.0014, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 99.4631, NLL-Loss 99.4620, KL-Loss 0.0011, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 97.6005, NLL-Loss 97.5966, KL-Loss 0.0039, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 87.6586, NLL-Loss 87.6626, KL-Loss -0.0040, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 96.5901, NLL-Loss 96.5858, KL-Loss 0.0044, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 81.0698, NLL-Loss 81.0639, KL-Loss 0.0059, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 112.9098, NLL-Loss 112.9085, KL-Loss 0.0013, KL-Weight 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0550/1315, ELBO-Loss 103.4210, NLL-Loss 103.4490, KL-Loss -0.0280, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 93.6877, NLL-Loss 93.6737, KL-Loss 0.0140, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 103.3740, NLL-Loss 103.3622, KL-Loss 0.0119, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 103.7814, NLL-Loss 103.7788, KL-Loss 0.0026, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 96.9742, NLL-Loss 96.9649, KL-Loss 0.0093, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 95.9106, NLL-Loss 95.9058, KL-Loss 0.0048, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 104.1677, NLL-Loss 104.1617, KL-Loss 0.0059, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 95.8665, NLL-Loss 95.8717, KL-Loss -0.0052, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 91.6836, NLL-Loss 91.6843, KL-Loss -0.0007, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 112.6547, NLL-Loss 112.6523, KL-Loss 0.0024, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 106.2053, NLL-Loss 106.2045, KL-Loss 0.0007, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 104.5047, NLL-Loss 104.4957, KL-Loss 0.0090, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 96.1815, NLL-Loss 96.1786, KL-Loss 0.0029, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 79.6850, NLL-Loss 79.6586, KL-Loss 0.0263, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 100.7049, NLL-Loss 100.6751, KL-Loss 0.0299, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 94.5575, NLL-Loss 94.5303, KL-Loss 0.0272, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 106.3966, NLL-Loss 106.3980, KL-Loss -0.0014, KL-Weight 1.0000\n",
      "TRAIN Epoch 11/20, ELBO 95.9611, NLL 95.9579, KL 95.9579, PPL 1.0002\n",
      "VALID Epoch 11/20, ELBO 107.7203, NLL 107.7203, KL 107.7203, PPL 1.0000\n",
      "TEST Epoch 11/20, ELBO 106.2904, NLL 106.2876, KL 106.2876, PPL 1.0001\n",
      "Model saved at vamp05-23/4:20/E11.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 88.7740, NLL-Loss 88.7790, KL-Loss -0.0050, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 103.4392, NLL-Loss 103.4521, KL-Loss -0.0130, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 96.4221, NLL-Loss 96.4174, KL-Loss 0.0046, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 109.5725, NLL-Loss 109.5744, KL-Loss -0.0019, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 72.6543, NLL-Loss 72.6250, KL-Loss 0.0293, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 94.2048, NLL-Loss 94.1947, KL-Loss 0.0101, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 96.8119, NLL-Loss 96.8153, KL-Loss -0.0035, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 92.9095, NLL-Loss 92.9116, KL-Loss -0.0021, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 94.5394, NLL-Loss 94.5304, KL-Loss 0.0090, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 94.9034, NLL-Loss 94.9070, KL-Loss -0.0036, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 102.5184, NLL-Loss 102.5193, KL-Loss -0.0009, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 100.2424, NLL-Loss 100.2358, KL-Loss 0.0067, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 96.0889, NLL-Loss 96.0857, KL-Loss 0.0032, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 93.1909, NLL-Loss 93.1829, KL-Loss 0.0080, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 95.0111, NLL-Loss 95.0157, KL-Loss -0.0045, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 81.3349, NLL-Loss 81.4024, KL-Loss -0.0676, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 105.3514, NLL-Loss 105.3486, KL-Loss 0.0027, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 97.8857, NLL-Loss 97.8820, KL-Loss 0.0037, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 106.9892, NLL-Loss 106.9865, KL-Loss 0.0026, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 102.5648, NLL-Loss 102.5561, KL-Loss 0.0087, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 86.3157, NLL-Loss 86.3086, KL-Loss 0.0071, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 95.8548, NLL-Loss 95.8429, KL-Loss 0.0118, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 90.7800, NLL-Loss 90.7883, KL-Loss -0.0083, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 87.2399, NLL-Loss 87.2403, KL-Loss -0.0004, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 90.0292, NLL-Loss 90.0025, KL-Loss 0.0267, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 96.1782, NLL-Loss 96.1768, KL-Loss 0.0014, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 86.3883, NLL-Loss 86.4039, KL-Loss -0.0155, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 87.2324, NLL-Loss 87.2328, KL-Loss -0.0004, KL-Weight 1.0000\n",
      "TRAIN Epoch 12/20, ELBO 94.9330, NLL 94.9300, KL 94.9300, PPL 1.0001\n",
      "VALID Epoch 12/20, ELBO 107.6265, NLL 107.6244, KL 107.6244, PPL 1.0001\n",
      "TEST Epoch 12/20, ELBO 106.1929, NLL 106.1922, KL 106.1922, PPL 1.0000\n",
      "Model saved at vamp05-23/4:20/E12.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 78.9560, NLL-Loss 78.9530, KL-Loss 0.0030, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 97.5831, NLL-Loss 97.5746, KL-Loss 0.0084, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 92.9930, NLL-Loss 92.9770, KL-Loss 0.0160, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 103.3734, NLL-Loss 103.3750, KL-Loss -0.0016, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 80.4769, NLL-Loss 80.4759, KL-Loss 0.0010, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 92.8071, NLL-Loss 92.8105, KL-Loss -0.0034, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 97.4812, NLL-Loss 97.4723, KL-Loss 0.0090, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 98.1710, NLL-Loss 98.1725, KL-Loss -0.0015, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 87.1757, NLL-Loss 87.1773, KL-Loss -0.0016, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 102.2447, NLL-Loss 102.2467, KL-Loss -0.0019, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 97.7532, NLL-Loss 97.7481, KL-Loss 0.0051, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 80.7429, NLL-Loss 80.7353, KL-Loss 0.0075, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 89.2796, NLL-Loss 89.2782, KL-Loss 0.0014, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 113.3053, NLL-Loss 113.2990, KL-Loss 0.0064, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 107.8974, NLL-Loss 107.8971, KL-Loss 0.0002, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 105.4796, NLL-Loss 105.4373, KL-Loss 0.0423, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 99.2270, NLL-Loss 99.2251, KL-Loss 0.0019, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 99.7952, NLL-Loss 99.8054, KL-Loss -0.0102, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 96.2151, NLL-Loss 96.2111, KL-Loss 0.0040, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 94.1490, NLL-Loss 94.1478, KL-Loss 0.0012, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 95.9883, NLL-Loss 95.9902, KL-Loss -0.0019, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 91.2135, NLL-Loss 91.2175, KL-Loss -0.0040, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 85.4156, NLL-Loss 85.3885, KL-Loss 0.0271, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 97.0890, NLL-Loss 97.0799, KL-Loss 0.0091, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 91.8905, NLL-Loss 91.8916, KL-Loss -0.0011, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 110.3744, NLL-Loss 110.3639, KL-Loss 0.0105, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 97.6383, NLL-Loss 97.6473, KL-Loss -0.0090, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 84.8693, NLL-Loss 84.8646, KL-Loss 0.0047, KL-Weight 1.0000\n",
      "TRAIN Epoch 13/20, ELBO 94.5639, NLL 94.5620, KL 94.5620, PPL 1.0001\n",
      "VALID Epoch 13/20, ELBO 107.6420, NLL 107.6392, KL 107.6392, PPL 1.0001\n",
      "TEST Epoch 13/20, ELBO 106.1599, NLL 106.1587, KL 106.1587, PPL 1.0001\n",
      "Model saved at vamp05-23/4:20/E13.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 108.4408, NLL-Loss 108.4542, KL-Loss -0.0134, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 95.4252, NLL-Loss 95.4259, KL-Loss -0.0008, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 102.8603, NLL-Loss 102.8584, KL-Loss 0.0019, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 83.2453, NLL-Loss 83.2601, KL-Loss -0.0148, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 95.2965, NLL-Loss 95.2931, KL-Loss 0.0033, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 90.9800, NLL-Loss 90.9625, KL-Loss 0.0175, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 86.6624, NLL-Loss 86.6622, KL-Loss 0.0002, KL-Weight 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0350/1315, ELBO-Loss 100.6967, NLL-Loss 100.6953, KL-Loss 0.0015, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 90.5462, NLL-Loss 90.5475, KL-Loss -0.0012, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 85.8760, NLL-Loss 85.8774, KL-Loss -0.0014, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 82.5793, NLL-Loss 82.5670, KL-Loss 0.0124, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 102.4391, NLL-Loss 102.4404, KL-Loss -0.0013, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 88.6633, NLL-Loss 88.6789, KL-Loss -0.0156, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 83.8268, NLL-Loss 83.8225, KL-Loss 0.0043, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 98.4911, NLL-Loss 98.4944, KL-Loss -0.0033, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 96.1212, NLL-Loss 96.1315, KL-Loss -0.0102, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 121.4938, NLL-Loss 121.4975, KL-Loss -0.0037, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 93.7581, NLL-Loss 93.7485, KL-Loss 0.0096, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 93.9663, NLL-Loss 93.9656, KL-Loss 0.0008, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 100.3741, NLL-Loss 100.3716, KL-Loss 0.0025, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 92.3520, NLL-Loss 92.3500, KL-Loss 0.0019, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 104.5622, NLL-Loss 104.5678, KL-Loss -0.0056, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 89.9013, NLL-Loss 89.8997, KL-Loss 0.0016, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 84.5053, NLL-Loss 84.4985, KL-Loss 0.0068, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 95.8475, NLL-Loss 95.8487, KL-Loss -0.0012, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 83.2950, NLL-Loss 83.2970, KL-Loss -0.0020, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 92.7972, NLL-Loss 92.7880, KL-Loss 0.0092, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 84.0513, NLL-Loss 84.0773, KL-Loss -0.0260, KL-Weight 1.0000\n",
      "TRAIN Epoch 14/20, ELBO 94.0036, NLL 94.0023, KL 94.0023, PPL 1.0001\n",
      "VALID Epoch 14/20, ELBO 107.6619, NLL 107.6600, KL 107.6600, PPL 1.0001\n",
      "TEST Epoch 14/20, ELBO 106.1628, NLL 106.1634, KL 106.1634, PPL 1.0000\n",
      "Model saved at vamp05-23/4:20/E14.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 99.7253, NLL-Loss 99.7298, KL-Loss -0.0045, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 65.4286, NLL-Loss 65.3921, KL-Loss 0.0366, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 89.6462, NLL-Loss 89.6453, KL-Loss 0.0009, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 90.2734, NLL-Loss 90.2625, KL-Loss 0.0109, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 91.4494, NLL-Loss 91.4330, KL-Loss 0.0164, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 95.2995, NLL-Loss 95.2920, KL-Loss 0.0075, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 93.0217, NLL-Loss 93.0141, KL-Loss 0.0076, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 103.0244, NLL-Loss 102.9563, KL-Loss 0.0681, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 90.8280, NLL-Loss 90.8291, KL-Loss -0.0011, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 94.2231, NLL-Loss 94.2230, KL-Loss 0.0001, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 79.5271, NLL-Loss 79.5307, KL-Loss -0.0036, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 101.2970, NLL-Loss 101.2691, KL-Loss 0.0278, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 94.7338, NLL-Loss 94.7363, KL-Loss -0.0025, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 93.5950, NLL-Loss 93.5941, KL-Loss 0.0009, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 78.4975, NLL-Loss 78.4908, KL-Loss 0.0067, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 96.5145, NLL-Loss 96.5052, KL-Loss 0.0092, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 104.1184, NLL-Loss 104.1326, KL-Loss -0.0142, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 101.6004, NLL-Loss 101.6491, KL-Loss -0.0487, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 102.5169, NLL-Loss 102.5140, KL-Loss 0.0029, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 106.6258, NLL-Loss 106.6182, KL-Loss 0.0077, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 103.9050, NLL-Loss 103.8944, KL-Loss 0.0106, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 87.5896, NLL-Loss 87.5879, KL-Loss 0.0017, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 91.0391, NLL-Loss 91.0418, KL-Loss -0.0028, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 95.7582, NLL-Loss 95.7556, KL-Loss 0.0026, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 93.8124, NLL-Loss 93.8101, KL-Loss 0.0023, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 106.4974, NLL-Loss 106.5160, KL-Loss -0.0186, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 97.3675, NLL-Loss 97.3587, KL-Loss 0.0088, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 102.0611, NLL-Loss 102.0648, KL-Loss -0.0036, KL-Weight 1.0000\n",
      "TRAIN Epoch 15/20, ELBO 93.8475, NLL 93.8456, KL 93.8456, PPL 1.0001\n",
      "VALID Epoch 15/20, ELBO 107.6810, NLL 107.6799, KL 107.6799, PPL 1.0001\n",
      "TEST Epoch 15/20, ELBO 106.1369, NLL 106.1357, KL 106.1357, PPL 1.0001\n",
      "Model saved at vamp05-23/4:20/E15.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 85.8784, NLL-Loss 85.8838, KL-Loss -0.0053, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 102.5247, NLL-Loss 102.5231, KL-Loss 0.0016, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 79.8170, NLL-Loss 79.8148, KL-Loss 0.0021, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 91.4558, NLL-Loss 91.4680, KL-Loss -0.0122, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 86.3546, NLL-Loss 86.3626, KL-Loss -0.0079, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 88.4747, NLL-Loss 88.4615, KL-Loss 0.0132, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 73.6928, NLL-Loss 73.6641, KL-Loss 0.0287, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 102.4540, NLL-Loss 102.4511, KL-Loss 0.0029, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 87.4821, NLL-Loss 87.4709, KL-Loss 0.0112, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 98.2548, NLL-Loss 98.2537, KL-Loss 0.0010, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 92.4530, NLL-Loss 92.4403, KL-Loss 0.0127, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 95.2923, NLL-Loss 95.2967, KL-Loss -0.0043, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 93.5784, NLL-Loss 93.5713, KL-Loss 0.0071, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 111.8031, NLL-Loss 111.7984, KL-Loss 0.0047, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 86.5189, NLL-Loss 86.5201, KL-Loss -0.0011, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 107.0460, NLL-Loss 107.0524, KL-Loss -0.0065, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 99.7224, NLL-Loss 99.7169, KL-Loss 0.0055, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 87.6759, NLL-Loss 87.6850, KL-Loss -0.0091, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 91.6023, NLL-Loss 91.6094, KL-Loss -0.0071, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 88.5335, NLL-Loss 88.5137, KL-Loss 0.0198, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 98.0946, NLL-Loss 98.0947, KL-Loss -0.0002, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 102.1966, NLL-Loss 102.2008, KL-Loss -0.0043, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 86.0494, NLL-Loss 86.0674, KL-Loss -0.0180, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 92.3465, NLL-Loss 92.3285, KL-Loss 0.0180, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 103.5240, NLL-Loss 103.5207, KL-Loss 0.0033, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 97.5558, NLL-Loss 97.5626, KL-Loss -0.0068, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 100.1256, NLL-Loss 100.1256, KL-Loss 0.0000, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 94.9342, NLL-Loss 94.9321, KL-Loss 0.0021, KL-Weight 1.0000\n",
      "TRAIN Epoch 16/20, ELBO 93.5459, NLL 93.5443, KL 93.5443, PPL 1.0001\n",
      "VALID Epoch 16/20, ELBO 107.6660, NLL 107.6633, KL 107.6633, PPL 1.0001\n",
      "TEST Epoch 16/20, ELBO 106.1304, NLL 106.1284, KL 106.1284, PPL 1.0001\n",
      "Model saved at vamp05-23/4:20/E16.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 87.8672, NLL-Loss 87.8641, KL-Loss 0.0031, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 94.0174, NLL-Loss 94.0158, KL-Loss 0.0016, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 88.9821, NLL-Loss 88.9868, KL-Loss -0.0047, KL-Weight 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0150/1315, ELBO-Loss 88.3603, NLL-Loss 88.3702, KL-Loss -0.0100, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 94.7240, NLL-Loss 94.7190, KL-Loss 0.0051, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 98.0038, NLL-Loss 98.0072, KL-Loss -0.0034, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 97.0719, NLL-Loss 97.0684, KL-Loss 0.0035, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 89.5511, NLL-Loss 89.5478, KL-Loss 0.0033, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 92.1688, NLL-Loss 92.1646, KL-Loss 0.0042, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 84.2734, NLL-Loss 84.2682, KL-Loss 0.0052, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 97.8152, NLL-Loss 97.7876, KL-Loss 0.0276, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 98.5993, NLL-Loss 98.6120, KL-Loss -0.0127, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 108.5388, NLL-Loss 108.5299, KL-Loss 0.0088, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 90.6431, NLL-Loss 90.6436, KL-Loss -0.0006, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 105.1631, NLL-Loss 105.1639, KL-Loss -0.0007, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 99.5475, NLL-Loss 99.5402, KL-Loss 0.0073, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 92.7147, NLL-Loss 92.7170, KL-Loss -0.0023, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 85.7921, NLL-Loss 85.7375, KL-Loss 0.0546, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 89.4569, NLL-Loss 89.4278, KL-Loss 0.0291, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 88.7288, NLL-Loss 88.7225, KL-Loss 0.0062, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 101.3585, NLL-Loss 101.3650, KL-Loss -0.0065, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 96.4573, NLL-Loss 96.4331, KL-Loss 0.0241, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 80.0850, NLL-Loss 80.0862, KL-Loss -0.0012, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 85.9113, NLL-Loss 85.8970, KL-Loss 0.0143, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 100.4719, NLL-Loss 100.4656, KL-Loss 0.0063, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 95.9369, NLL-Loss 95.9342, KL-Loss 0.0027, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 76.6996, NLL-Loss 76.6955, KL-Loss 0.0040, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 76.6867, NLL-Loss 76.6798, KL-Loss 0.0069, KL-Weight 1.0000\n",
      "TRAIN Epoch 17/20, ELBO 93.4522, NLL 93.4505, KL 93.4505, PPL 1.0001\n",
      "VALID Epoch 17/20, ELBO 107.6823, NLL 107.6792, KL 107.6792, PPL 1.0001\n",
      "TEST Epoch 17/20, ELBO 106.1376, NLL 106.1371, KL 106.1371, PPL 1.0000\n",
      "Model saved at vamp05-23/4:20/E17.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 87.4333, NLL-Loss 87.4326, KL-Loss 0.0007, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 87.8995, NLL-Loss 87.8943, KL-Loss 0.0052, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 84.8955, NLL-Loss 84.8906, KL-Loss 0.0049, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 88.0967, NLL-Loss 88.0917, KL-Loss 0.0050, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 104.1434, NLL-Loss 104.1332, KL-Loss 0.0102, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 88.6289, NLL-Loss 88.6314, KL-Loss -0.0026, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 100.8448, NLL-Loss 100.8323, KL-Loss 0.0125, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 88.6821, NLL-Loss 88.6829, KL-Loss -0.0009, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 94.6909, NLL-Loss 94.6934, KL-Loss -0.0024, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 98.2524, NLL-Loss 98.2505, KL-Loss 0.0019, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 82.5894, NLL-Loss 82.6005, KL-Loss -0.0111, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 103.7542, NLL-Loss 103.7542, KL-Loss 0.0000, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 115.3820, NLL-Loss 115.3653, KL-Loss 0.0166, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 87.1244, NLL-Loss 87.1262, KL-Loss -0.0017, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 92.8484, NLL-Loss 92.8495, KL-Loss -0.0012, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 90.6360, NLL-Loss 90.5775, KL-Loss 0.0585, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 82.8881, NLL-Loss 82.8887, KL-Loss -0.0007, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 82.8944, NLL-Loss 82.8969, KL-Loss -0.0024, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 109.7840, NLL-Loss 109.7883, KL-Loss -0.0044, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 90.1451, NLL-Loss 90.1455, KL-Loss -0.0004, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 99.2558, NLL-Loss 99.2606, KL-Loss -0.0048, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 86.5252, NLL-Loss 86.5261, KL-Loss -0.0009, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 84.4473, NLL-Loss 84.4426, KL-Loss 0.0047, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 85.5958, NLL-Loss 85.5920, KL-Loss 0.0038, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 94.3646, NLL-Loss 94.3672, KL-Loss -0.0026, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 106.9741, NLL-Loss 106.9797, KL-Loss -0.0056, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 93.3888, NLL-Loss 93.3885, KL-Loss 0.0003, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 102.9247, NLL-Loss 102.9216, KL-Loss 0.0031, KL-Weight 1.0000\n",
      "TRAIN Epoch 18/20, ELBO 93.2967, NLL 93.2952, KL 93.2952, PPL 1.0001\n",
      "VALID Epoch 18/20, ELBO 107.6748, NLL 107.6721, KL 107.6721, PPL 1.0001\n",
      "TEST Epoch 18/20, ELBO 106.1157, NLL 106.1158, KL 106.1158, PPL 1.0000\n",
      "Model saved at vamp05-23/4:20/E18.pkl\n",
      "\n",
      "TRAIN Batch 0000/1315, ELBO-Loss 86.7215, NLL-Loss 86.7196, KL-Loss 0.0019, KL-Weight 1.0000\n",
      "TRAIN Batch 0050/1315, ELBO-Loss 109.5499, NLL-Loss 109.5473, KL-Loss 0.0026, KL-Weight 1.0000\n",
      "TRAIN Batch 0100/1315, ELBO-Loss 79.7078, NLL-Loss 79.7444, KL-Loss -0.0367, KL-Weight 1.0000\n",
      "TRAIN Batch 0150/1315, ELBO-Loss 109.3409, NLL-Loss 109.3428, KL-Loss -0.0019, KL-Weight 1.0000\n",
      "TRAIN Batch 0200/1315, ELBO-Loss 96.4307, NLL-Loss 96.4162, KL-Loss 0.0144, KL-Weight 1.0000\n",
      "TRAIN Batch 0250/1315, ELBO-Loss 102.5410, NLL-Loss 102.5353, KL-Loss 0.0056, KL-Weight 1.0000\n",
      "TRAIN Batch 0300/1315, ELBO-Loss 89.0637, NLL-Loss 89.0647, KL-Loss -0.0010, KL-Weight 1.0000\n",
      "TRAIN Batch 0350/1315, ELBO-Loss 82.8776, NLL-Loss 82.8760, KL-Loss 0.0016, KL-Weight 1.0000\n",
      "TRAIN Batch 0400/1315, ELBO-Loss 103.6978, NLL-Loss 103.6999, KL-Loss -0.0021, KL-Weight 1.0000\n",
      "TRAIN Batch 0450/1315, ELBO-Loss 94.7875, NLL-Loss 94.7844, KL-Loss 0.0032, KL-Weight 1.0000\n",
      "TRAIN Batch 0500/1315, ELBO-Loss 100.8458, NLL-Loss 100.8445, KL-Loss 0.0013, KL-Weight 1.0000\n",
      "TRAIN Batch 0550/1315, ELBO-Loss 97.7845, NLL-Loss 97.7786, KL-Loss 0.0059, KL-Weight 1.0000\n",
      "TRAIN Batch 0600/1315, ELBO-Loss 91.5167, NLL-Loss 91.5135, KL-Loss 0.0032, KL-Weight 1.0000\n",
      "TRAIN Batch 0650/1315, ELBO-Loss 99.3102, NLL-Loss 99.3007, KL-Loss 0.0095, KL-Weight 1.0000\n",
      "TRAIN Batch 0700/1315, ELBO-Loss 102.1583, NLL-Loss 102.1535, KL-Loss 0.0047, KL-Weight 1.0000\n",
      "TRAIN Batch 0750/1315, ELBO-Loss 96.6528, NLL-Loss 96.6523, KL-Loss 0.0005, KL-Weight 1.0000\n",
      "TRAIN Batch 0800/1315, ELBO-Loss 97.3903, NLL-Loss 97.3870, KL-Loss 0.0032, KL-Weight 1.0000\n",
      "TRAIN Batch 0850/1315, ELBO-Loss 95.6792, NLL-Loss 95.6753, KL-Loss 0.0039, KL-Weight 1.0000\n",
      "TRAIN Batch 0900/1315, ELBO-Loss 97.8474, NLL-Loss 97.8469, KL-Loss 0.0005, KL-Weight 1.0000\n",
      "TRAIN Batch 0950/1315, ELBO-Loss 89.5305, NLL-Loss 89.5497, KL-Loss -0.0192, KL-Weight 1.0000\n",
      "TRAIN Batch 1000/1315, ELBO-Loss 108.9332, NLL-Loss 108.9324, KL-Loss 0.0007, KL-Weight 1.0000\n",
      "TRAIN Batch 1050/1315, ELBO-Loss 86.3377, NLL-Loss 86.3540, KL-Loss -0.0163, KL-Weight 1.0000\n",
      "TRAIN Batch 1100/1315, ELBO-Loss 78.1291, NLL-Loss 78.1297, KL-Loss -0.0005, KL-Weight 1.0000\n",
      "TRAIN Batch 1150/1315, ELBO-Loss 104.2328, NLL-Loss 104.2345, KL-Loss -0.0016, KL-Weight 1.0000\n",
      "TRAIN Batch 1200/1315, ELBO-Loss 89.2564, NLL-Loss 89.2592, KL-Loss -0.0028, KL-Weight 1.0000\n",
      "TRAIN Batch 1250/1315, ELBO-Loss 90.3613, NLL-Loss 90.3577, KL-Loss 0.0035, KL-Weight 1.0000\n",
      "TRAIN Batch 1300/1315, ELBO-Loss 80.7644, NLL-Loss 80.7619, KL-Loss 0.0025, KL-Weight 1.0000\n",
      "TRAIN Batch 1314/1315, ELBO-Loss 82.4751, NLL-Loss 82.4789, KL-Loss -0.0039, KL-Weight 1.0000\n",
      "TRAIN Epoch 19/20, ELBO 93.2586, NLL 93.2574, KL 93.2574, PPL 1.0001\n",
      "VALID Epoch 19/20, ELBO 107.6773, NLL 107.6765, KL 107.6765, PPL 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Epoch 19/20, ELBO 106.1170, NLL 106.1154, KL 106.1154, PPL 1.0001\n",
      "Model saved at vamp05-23/4:20/E19.pkl\n",
      "\n",
      "Total cost time 00 hr 31 min 39 sec\n"
     ]
    }
   ],
   "source": [
    "# training setting\n",
    "epoch = 20\n",
    "print_every = 50\n",
    "\n",
    "# training interface\n",
    "step = 0\n",
    "tracker = {'ELBO': [], 'NLL': [], 'KL': [], 'KL_weight': []}\n",
    "start_time = time.time()\n",
    "for ep in range(epoch):\n",
    "    # learning rate decay\n",
    "    if ep >= 10 and ep % 2 == 0:\n",
    "        learning_rate = learning_rate * 0.5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "    for split in splits:\n",
    "        dataloader = dataloaders[split]\n",
    "        model.train() if split == 'train' else model.eval()\n",
    "        totals = {'ELBO': 0., 'NLL': 0., 'KL': 0., 'words': 0}\n",
    "\n",
    "        for itr, (enc_inputs, dec_inputs, targets, lengths) in enumerate(dataloader):\n",
    "            bsize = enc_inputs.size(0)\n",
    "            enc_inputs = enc_inputs.to(device)\n",
    "            dec_inputs = dec_inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            # forward\n",
    "            logp, z_q, mu, logvar = model(enc_inputs, dec_inputs, lengths)\n",
    "\n",
    "            # calculate loss\n",
    "            NLL_loss = NLL(logp, targets, lengths + 1)\n",
    "            # KL loss\n",
    "            log_p_z = log_prior(z_q)\n",
    "            log_q_z = log_Normal_diag(z_q, mu, logvar, dim=1)\n",
    "            KL_loss = torch.sum(-(log_p_z - log_q_z))\n",
    "            KL_weight = 1.0\n",
    "            loss = (NLL_loss + KL_weight * KL_loss) / bsize\n",
    "            \n",
    "            # cumulate\n",
    "            totals['ELBO'] += loss.item() * bsize\n",
    "            totals['NLL'] += NLL_loss.item()\n",
    "            totals['KL'] += KL_loss.item()\n",
    "            totals['words'] += torch.sum(lengths).item()\n",
    "\n",
    "            # backward and optimize\n",
    "            if split == 'train':\n",
    "                step += 1\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "                optimizer.step()\n",
    "\n",
    "                # track\n",
    "                tracker['ELBO'].append(loss.item())\n",
    "                tracker['NLL'].append(NLL_loss.item() / bsize)\n",
    "                tracker['KL'].append(KL_loss.item() / bsize)\n",
    "                tracker['KL_weight'].append(KL_weight)\n",
    "\n",
    "                # print statistics\n",
    "                if itr % print_every == 0 or itr + 1 == len(dataloader):\n",
    "                    print(\"%s Batch %04d/%04d, ELBO-Loss %.4f, \"\n",
    "                          \"NLL-Loss %.4f, KL-Loss %.4f, KL-Weight %.4f\"\n",
    "                          % (split.upper(), itr, len(dataloader),\n",
    "                             tracker['ELBO'][-1], tracker['NLL'][-1],\n",
    "                             tracker['KL'][-1], tracker['KL_weight'][-1]))\n",
    "\n",
    "        samples = len(datasets[split])\n",
    "        print(\"%s Epoch %02d/%02d, ELBO %.4f, NLL %.4f, KL %.4f, PPL %.4f\"\n",
    "              % (split.upper(), ep, epoch, totals['ELBO'] / samples,\n",
    "                 totals['NLL'] / samples, totals['KL'] / samples,\n",
    "                 math.exp(totals['NLL'] / totals['words'])))\n",
    "\n",
    "    # save checkpoint\n",
    "    checkpoint_path = os.path.join(save_path, \"E%02d.pkl\" % ep)\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(\"Model saved at %s\\n\" % checkpoint_path)\n",
    "end_time = time.time()\n",
    "print('Total cost time',\n",
    "      time.strftime(\"%H hr %M min %S sec\", time.gmtime(end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAEhCAYAAAA0xARjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f87b33bc048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot KL curve\n",
    "fig, ax1 = plt.subplots()\n",
    "lns1 = ax1.plot(tracker['KL_weight'], 'b', label='KL term weight')\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('KL term weight')\n",
    "ax2 = ax1.twinx()\n",
    "lns2 = ax2.plot(tracker['KL'], 'r', label='KL term value')\n",
    "ax2.set_ylabel('KL term value')\n",
    "lns = lns1 + lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax1.legend(lns, labs, bbox_to_anchor=(0., 1.02, 1., .102),\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c225fc1cb293>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdec_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# latent space visualization\n",
    "features = np.empty([len(datasets['test']), latent_dim])\n",
    "for itr, (enc_inputs, dec_inputs, _, lengths) in enumerate(dataloaders['test']):\n",
    "    enc_inputs = enc_inputs.to(device)\n",
    "    dec_inputs = dec_inputs.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    _, mu, _ = model(enc_inputs, dec_inputs, lengths)\n",
    "    start, end = batch_size * itr, batch_size * (itr + 1)\n",
    "    features[start:end] = mu.data.cpu().numpy()\n",
    "tsne_z = TSNE(n_components=2).fit_transform(features)\n",
    "tracker['z'] = tsne_z\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(tsne_z[:, 0], tsne_z[:, 1], s=25, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save learning results\n",
    "sio.savemat(\"vanilla.mat\", tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
