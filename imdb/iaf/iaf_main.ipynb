{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.manifold import TSNE\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from imdb_data import IMDB\n",
    "from iaf_model import iaf\n",
    "from utils import linear_anneal, log_Normal_diag, log_Normal_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "max_len = 80\n",
    "batch_size = 32\n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "# Penn TreeBank (PTB) dataset\n",
    "data_path = '../data'\n",
    "datasets = {split: IMDB(root=data_path, split=split) for split in splits}\n",
    "\n",
    "# dataloader\n",
    "dataloaders = {split: DataLoader(datasets[split],\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=split=='train',\n",
    "                                    num_workers=cpu_count(),\n",
    "                                    pin_memory=torch.cuda.is_available())\n",
    "                                    for split in splits}\n",
    "\n",
    "symbols = datasets['train'].symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iaf model\n",
    "embedding_size = 300\n",
    "hidden_size = 256\n",
    "latent_dim = 32\n",
    "dropout_rate = 0.5\n",
    "model = iaf(vocab_size=datasets['train'].vocab_size,\n",
    "               embed_size=embedding_size,\n",
    "               time_step=max_len,\n",
    "               hidden_size=hidden_size,\n",
    "               z_dim=latent_dim,\n",
    "               dropout_rate=dropout_rate,\n",
    "               bos_idx=symbols['<bos>'],\n",
    "               eos_idx=symbols['<eos>'],\n",
    "               pad_idx=symbols['<pad>'],\n",
    "               n_comb = 2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder to save model\n",
    "if False:\n",
    "    save_path = 'iaf'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective function\n",
    "learning_rate = 0.001\n",
    "criterion = nn.NLLLoss(size_average=False, ignore_index=symbols['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion_c = nn.CrossEntropyLoss()\n",
    "\n",
    "# negative log likelihood\n",
    "def NLL(logp, target, length):\n",
    "    target = target[:, :torch.max(length).item()].contiguous().view(-1)\n",
    "    logp = logp.view(-1, logp.size(-1))\n",
    "    return criterion(logp, target)\n",
    "\n",
    "# KL divergence\n",
    "def KL_div(mu, logvar):\n",
    "    return -0.5 * torch.sum(1. + logvar - mu.pow(2) - logvar.exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training setting\n",
    "epoch = 20\n",
    "print_every = 50\n",
    "\n",
    "# training interface\n",
    "step = 0\n",
    "tracker = {'ELBO': [], 'NLL': [], 'KL': [], 'KL_weight': [], 'accuracy': []}\n",
    "start_time = time.time()\n",
    "for ep in range(epoch):\n",
    "    # learning rate decay\n",
    "    if ep >= 10 and ep % 2 == 0:\n",
    "        learning_rate = learning_rate * 0.5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "    for split in splits:\n",
    "        dataloader = dataloaders[split]\n",
    "        model.train() if split == 'train' else model.eval()\n",
    "        totals = {'ELBO': 0., 'NLL': 0., 'KL': 0., 'words': 0, 'correct': 0}\n",
    "        for itr, (enc_inputs, dec_inputs, targets, lengths, labels) in enumerate(dataloader):\n",
    "            bsize = enc_inputs.size(0)\n",
    "            enc_inputs = enc_inputs.to(device)\n",
    "            dec_inputs = dec_inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward\n",
    "            logp, z_0, z_T, mu, logvar, logc, labels = model(enc_inputs, dec_inputs, lengths, labels)\n",
    "            \n",
    "\n",
    "            # calculate loss\n",
    "            NLL_loss = NLL(logp, targets, lengths + 1)\n",
    "            # classification loss\n",
    "            #class_loss = nn.CrossEntropyLoss(logc, labels)\n",
    "            class_loss = criterion_c(logc, labels)\n",
    "            \n",
    "            \n",
    "            # KL loss\n",
    "            log_p_z = log_Normal_standard(z_T, dim=1)\n",
    "            log_q_z = log_Normal_diag(z_0, mu, logvar, dim=1)\n",
    "            KL_loss = torch.sum(-(log_p_z - log_q_z))\n",
    "            KL_weight = linear_anneal(step, len(dataloaders['train']) * 10)\n",
    "            loss = (NLL_loss + KL_weight * KL_loss) / bsize + class_loss         \n",
    "            \n",
    "            \n",
    "            # cumulate\n",
    "            totals['ELBO'] += loss.item() * bsize\n",
    "            totals['NLL'] += NLL_loss.item()\n",
    "            totals['KL'] += KL_loss.item()\n",
    "            totals['words'] += torch.sum(lengths).item()\n",
    "            _, predicted = torch.max(logc, -1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            totals['correct'] += correct\n",
    "\n",
    "            # backward and optimize\n",
    "            if split == 'train':\n",
    "                step += 1\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "                optimizer.step()\n",
    "\n",
    "                # track\n",
    "                tracker['ELBO'].append(loss.item())\n",
    "                tracker['NLL'].append(NLL_loss.item() / bsize)\n",
    "                tracker['KL'].append(KL_loss.item() / bsize)\n",
    "                tracker['KL_weight'].append(KL_weight)\n",
    "                \n",
    "                if(False):\n",
    "                    # print statistics\n",
    "                    if itr % print_every == 0 or itr + 1 == len(dataloader):\n",
    "                        print(\"%s Batch %04d/%04d, ELBO-Loss %.4f, \"\n",
    "                              \"NLL-Loss %.4f, KL-Loss %.4f, KL-Weight %.4f,\"\n",
    "                              \"Loss %.4f, Accuracy %2.2f\"\n",
    "                              % (split.upper(), itr, len(dataloader),\n",
    "                                 tracker['ELBO'][-1], tracker['NLL'][-1],\n",
    "                                 tracker['KL'][-1], tracker['KL_weight'][-1],\n",
    "                                 class_loss, correct / bsize * 100))\n",
    "            #break\n",
    "        #break\n",
    "\n",
    "        samples = len(datasets[split])\n",
    "        print(\"%s Epoch %02d/%02d, ELBO %.4f, NLL %.4f, KL %.4f, PPL %.4f, Accuracy %2.2f\"\n",
    "              % (split.upper(), ep, epoch, totals['ELBO'] / samples,\n",
    "                 totals['NLL'] / samples, totals['KL'] / samples,\n",
    "                 math.exp(totals['NLL'] / totals['words']), totals['correct'] / samples * 100))\n",
    "    print('\\n')\n",
    "\n",
    "    #break\n",
    "    # save checkpoint\n",
    "    #checkpoint_path = os.path.join(save_path, \"E%02d.pkl\" % ep)\n",
    "    #torch.save(model.state_dict(), checkpoint_path)\n",
    "    #print(\"Model saved at %s\\n\" % checkpoint_path)\n",
    "end_time = time.time()\n",
    "print('Total cost time',\n",
    "      time.strftime(\"%H hr %M min %S sec\", time.gmtime(end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another KL\n",
    "\n",
    "totals = {'ELBO': 0., 'NLL': 0., 'KL': 0., 'words': 0}\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for itr, (enc_inputs, dec_inputs, targets, lengths, labels) in enumerate(dataloaders['test']):\n",
    "        bsize = enc_inputs.size(0)\n",
    "        enc_inputs = enc_inputs.to(device)\n",
    "        dec_inputs = dec_inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        logp, z_0, z_T, mu, logvar, _, _ = model(enc_inputs, dec_inputs, lengths, labels)\n",
    "\n",
    "        # calculate loss\n",
    "        NLL_loss = NLL(logp, targets, lengths + 1)\n",
    "        # KL loss\n",
    "        log_p_z = log_Normal_standard(z_0, dim=1)\n",
    "        log_q_z = log_Normal_diag(z_0, mu, logvar, dim=1)\n",
    "        KL_loss = torch.sum(log_q_z - log_p_z)\n",
    "        #KL_weight = linear_anneal(step, len(dataloaders['train']) * 10)\n",
    "        KL_weight = 1\n",
    "        loss = (NLL_loss + KL_weight * KL_loss) / bsize\n",
    "\n",
    "        # cumulate\n",
    "        totals['ELBO'] += loss.item() * bsize\n",
    "        totals['NLL'] += NLL_loss.item()\n",
    "        totals['KL'] += KL_loss.item()\n",
    "        totals['words'] += torch.sum(lengths).item()\n",
    "    samples = len(datasets['test'])\n",
    "    print(\"%s Epoch %02d/%02d, ELBO %.4f, NLL %.4f, KL %.4f, PPL %.4f\"\n",
    "          % (split.upper(), ep, epoch, totals['ELBO'] / samples,\n",
    "             totals['NLL'] / samples, totals['KL'] / samples,\n",
    "             math.exp(totals['NLL'] / totals['words'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate au\n",
    "\n",
    "delta = 0.01\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    cnt = 0\n",
    "    for itr, (enc_inputs, dec_inputs, targets, lengths, labels) in enumerate(dataloaders['test']):\n",
    "        bsize = enc_inputs.size(0)\n",
    "        enc_inputs = enc_inputs.to(device)\n",
    "        dec_inputs = dec_inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        logp, z_0, z_T, mu, logvar, _, _ = model(enc_inputs, dec_inputs, lengths, labels)\n",
    "        \n",
    "        if cnt == 0:\n",
    "            mu_sum = mu.sum(dim=0, keepdim=True)\n",
    "        else:\n",
    "            mu_sum = mu_sum + mu.sum(dim=0, keepdim=True)\n",
    "        cnt += mu.size(0)\n",
    "        \n",
    "    mu_mean = mu_sum / cnt\n",
    "        \n",
    "    cnt = 0\n",
    "    for itr, (enc_inputs, dec_inputs, targets, lengths, labels) in enumerate(dataloaders['test']):\n",
    "        bsize = enc_inputs.size(0)\n",
    "        enc_inputs = enc_inputs.to(device)\n",
    "        dec_inputs = dec_inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        logp, z_0, z_T, mu, logvar, _, _ = model(enc_inputs, dec_inputs, lengths, labels)\n",
    "        \n",
    "        if cnt == 0:\n",
    "            var_sum = ((mu - mu_mean) ** 2).sum(dim=0)\n",
    "        else:\n",
    "            var_sum = var_sum + ((mu - mu_mean) ** 2).sum(dim=0)\n",
    "        cnt += mu.size(0)\n",
    "        \n",
    "    au_var = var_sum / (cnt - 1)\n",
    "    \n",
    "    print((au_var >= delta).sum().item())\n",
    "    #print(au_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot KL curve\n",
    "if(True):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    lns1 = ax1.plot(tracker['KL_weight'], 'b', label='KL term weight')\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.set_xlabel('Step')\n",
    "    ax1.set_ylabel('KL term weight')\n",
    "    ax2 = ax1.twinx()\n",
    "    lns2 = ax2.plot(tracker['KL'], 'r', label='KL term value')\n",
    "    ax2.set_ylabel('KL term value')\n",
    "    lns = lns1 + lns2\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, bbox_to_anchor=(0., 1.02, 1., .102),\n",
    "               ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent space visualization\n",
    "if(True):\n",
    "    features = np.empty([len(datasets['test']), latent_dim])\n",
    "    feat_label = np.empty(len(datasets['test']))\n",
    "    for itr, (enc_inputs, dec_inputs, _, lengths, labels) in enumerate(dataloaders['test']):\n",
    "        enc_inputs = enc_inputs.to(device)\n",
    "        dec_inputs = dec_inputs.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        labels = labels.to(device)\n",
    "        _, _, _, mu, _, _, labels = model(enc_inputs, dec_inputs, lengths, labels)\n",
    "        start, end = batch_size * itr, batch_size * (itr + 1)\n",
    "        features[start:end] = mu.data.cpu().numpy()\n",
    "        feat_label[start:end] = labels.data.cpu().numpy()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_z = TSNE(n_components=2,perplexity=100).fit_transform(features)\n",
    "tracker['z'] = tsne_z\n",
    "tracker['label'] = feat_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(2):\n",
    "    plt.scatter(tsne_z[np.where(feat_label == i), 0], tsne_z[np.where(feat_label == i), 1], s=10, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save learning results\n",
    "sio.savemat(\"iaf.mat\", tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
